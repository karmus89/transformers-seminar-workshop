{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the fine-tuned BERT model\n",
    "\n",
    "In the previous notebook, we walked through the process of fine-tuning a pre-trained BERT model using masked language modeling, and saved the fine-tuned model for later use. \n",
    "\n",
    "In this notebook, we will continue our exploration of BERT by using the fine-tuned model to perform classification on a medical text dataset. Specifically, we will define a custom classification head on top of the fine-tuned BERT model using PyTorch, and train and evaluate the classification model on a held-out test set of medical text data. \n",
    "\n",
    "We will also compare the performance of our fine-tuned model to an out-of-the-box BERT model, which has not been fine-tuned on the medical text dataset. By the end of this notebook, you will have a solid understanding of how to use a fine-tuned BERT model for classification of medical text data, and how to compare its performance to an out-of-the-box BERT model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the fine-tuned BERT model\n",
    "\n",
    "To use the fine-tuned BERT model for classification, we can load the saved model from the previous notebook using the ``BertForMaskedLM`` class from the Hugging Face Transformers library. This class is the same pre-trained BERT model that we fine-tuned using masked language modeling in the previous notebook, and does not include a classification head.\n",
    "\n",
    "To attach our own classification head to the fine-tuned BERT model, we can define a custom classification head using PyTorch, which we will do in the next section. Once the classification head is defined, we can combine it with the fine-tuned BERT model using the PyTorch ``nn.Sequential`` module. We can then move the combined model to the device we plan to use for training and evaluation, such as a GPU if available."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load a pre-trained ``bert-base-uncased`` tokenizer to ``tokenizer`` variable.\n",
    ">\n",
    "> Load the fine-tuned ``bert-base-uncased-finetuned`` from ``\"../../model\"`` to ``model`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_finetuned = BertForMaskedLM.from_pretrained(\n",
    "    \"../../model/bert-base-uncased-finetuned\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load the processed data to ``df_train`` and ``df_validation``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"../../data/train.csv\", index_col=0)\n",
    "df_validation = pd.read_csv(\"../../data/validation.csv\", index_col=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dataset\n",
    "\n",
    "Before proceeding to training our model, we must first define a ``Dataset``. It needs to provide the model with inputs and target class label values. Let's begin with our inputs. We need to feed tokens to our model. Let's, therefore, transform our sentences to lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer(\n",
    "    df_train[\"transcription\"].values.tolist(),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "x_validation = tokenizer(\n",
    "    df_validation[\"transcription\"].values.tolist(),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go for the target values. Let's check once more that what classes we have in our data. Those are the values of the ``\"medical_specialty\"`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Surgery',\n",
       " 'Radiology',\n",
       " 'Consult',\n",
       " 'Cardiovascular',\n",
       " 'Orthopedic',\n",
       " 'General Medicine']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"medical_specialty\"].unique().tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our labels being in text, we need to map the labels to integers for our model. Let's create a label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_label = {\n",
    "    \"Surgery\": 0,\n",
    "    \"Radiology\": 1,\n",
    "    \"Consult\": 2,\n",
    "    \"Cardiovascular\": 3,\n",
    "    \"Orthopedic\": 4,\n",
    "    \"General Medicine\": 5,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then create a new ``\"label\"`` column to our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"medical_specialty\"].map(class_to_label).values\n",
    "y_validation = df_validation[\"medical_specialty\"].map(class_to_label).values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then define our dataset. We'll leave our target values as indices, since the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) expects the labels as indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MedicalTranscriptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        x[\"labels\"] = self.labels[idx]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap here a bit. The thing to note here is that the output of our dataset must conform to what is expected by the BERT model. This needs to be taken into account especially when using the Hugging Face ``Trainer`` to train the model. From the documentation of the [BertFormMaskedLM.forward](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertForMaskedLM.forward) we can see that the model expects _at least_ the following arguments:\n",
    " - ``input_ids`` of shape ``[batch, sequence_length]`` (required)\n",
    " - ``attention`` of shape ``[batch, sequence_length]`` (optional)\n",
    " - ``token_type_ids`` of shape ``[batch, sequence_length]`` (optional)\n",
    " - ``labels`` of shape ``[batch, sequence_length]`` (optional)\n",
    "\n",
    "For the last one, ``labels``, the ``BertForMaskedLM`` model expects per-token labels but this time our labels are per-sentence. For sentence classification we will thus omit passing the per-sentence labels by design as our labels are per-sentence and not per-token. The per-sentence labels will be used with the separately defined classification head. For each batch of samples, the ``Trainer`` passes the data batched under each keyword as a separate keyword argument for the model's ``forward`` function. That is why the ``labels`` have still to be included in our samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then see what the ``MedicalTranscriptionDataset`` actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample type: <class 'dict'>\n",
      "Sample keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 11360, 27011,  2187,  3244,  6612,  2656,  2095,  2214,  2381,\n",
       "         18549,  7749,  3653, 29021,  2260,  2322,  2384,  4531, 10514, 18098,\n",
       "          3022, 26787,  5809,  7166,  5740,  6190,  2302,  2440, 14983,  7697,\n",
       "          6578, 11917,  2128,  6494,  7542, 13472,  2012, 18981, 10536,  2186,\n",
       "          8746,  3746,  1018,  1020,  3671,  1999, 27843, 13102,  3981,  5809,\n",
       "          4942, 15782, 14289,  8017,  2483,  7166,  2239,  3671,  2146, 27947,\n",
       "          7166,  2239,  2306, 12170,  6895, 23270,  2389, 14100, 23828,  4942,\n",
       "         25148,  3370,  7166,  2239, 18323, 20368,  7941, 25641,  7166,  5740,\n",
       "          6190, 26721, 17695, 23722,  2906,  4664,  7166,  2239,  7704, 13311,\n",
       "          3143,  7697, 12532, 16778, 11231,  3012, 27947,  8133, 10109,  2186,\n",
       "          8746,  3746,  1018,  1021,  2186,  9402,  3746,  2184,  2570,  2312,\n",
       "          2940, 22818, 19583,  5994,  2471,  2972, 15219,  2431, 20368,  7941,\n",
       "          2132,  2186,  9402,  3746,  2410,  2539,  3378,  2312, 14092, 22678,\n",
       "          2924,  8445,  4649,  3258,  5468,  3155,  2321,  1060,  2324,  7382,\n",
       "          9706, 13675,  7088, 24755, 14066,  2140,  9812,  4254,  3258, 28424,\n",
       "          2186,  9645,  3746,  2184,  2403,  2186,  9402,  3746,  2324,  2654,\n",
       "         23828, 14092, 13508, 15778,  3674,  6970,  8445, 21412,  2303,  2089,\n",
       "          9808,  2618, 11663, 15422, 21716, 10610,  2271,  2089,  9808,  3366,\n",
       "          3560,  9854,  1022,  7382,  6705,  3365,  4175,  4417, 10917,  2012,\n",
       "          6528, 14505,  2181,  4317,  7406, 14092,  2690,  8904, 11631, 17897,\n",
       "          7941, 25641, 11892, 11888,  7697, 11228,  4892, 12532, 16778, 11231,\n",
       "          3012,  7645,  5292, 23296,  4649,  3258,  2186,  8746,  3746,  1019,\n",
       "          2184,  3671,  6020,  8904, 11631, 17897,  7941, 25641, 14308,  7697,\n",
       "          3671,  9353, 21716,  3695, 20464, 18891, 15431,  4101,  2302, 21978,\n",
       "          4942,  6305, 21716,  4818,  2686,  3671, 17195,  3597,  6305, 21716,\n",
       "          4818, 17195,  3597, 28600, 21673, 17195,  3597, 20464, 18891, 15431,\n",
       "         25641,  8331,  8904, 11631, 17897,  7941,  4101, 27947,  7166,  2239,\n",
       "         21867,  8605,  2312,  2940, 22818, 19583,  5994, 15219,  2431, 20368,\n",
       "          7941,  2132,  3378,  2312, 14092, 23828, 12936,  9808,  3366,  3560,\n",
       "          2924,  8445,  4649,  3258,  3674, 26721,  8445, 21412,  2303,  7704,\n",
       "          7697, 14092,  2690,  8904, 11631, 17897,  7941, 25641, 23828,  4942,\n",
       "         25148,  3370,  2146, 27947,  7166,  2239, 18323, 20368,  7941, 25641,\n",
       "          7704, 13311, 26721, 17695, 23722,  2906,  4664,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MedicalTranscriptionDataset(encodings=x_train, labels=y_train)\n",
    "sample = dataset[1]\n",
    "print(f\"Sample type: {type(sample)}\")\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "sample\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is important to notice that the dataset's samples must conform to the format expected by our underlying BERT model. \n",
    "\n",
    "Let's finally try to feed a sample to the model to see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output keys: odict_keys(['logits', 'hidden_states'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batchify\n",
    "batched_sample = {}\n",
    "for key, value in sample.items():\n",
    "    batched_sample[key] = value.unsqueeze(0)\n",
    "\n",
    "model_finetuned.to(\"cpu\")\n",
    "\n",
    "batched_sample.pop(\"labels\")\n",
    "sample_output = model_finetuned(**batched_sample, output_hidden_states=True)\n",
    "print(f\"Sample output keys: {sample_output.keys()}\")\n",
    "sample_output.hidden_states[-1][:, 0, :].shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the classification head\n",
    "\n",
    "To perform classification on the fine-tuned BERT model, we need to attach a custom classification head on top of the pre-trained model. This classification head will take the output embeddings from the fine-tuned BERT model and map them to a set of output classes using a feedforward neural network.\n",
    "\n",
    "To define the classification head, we can use PyTorch to create a new ``nn.Sequential`` module. The classification head should include at least one hidden layer with a ReLU activation function, as well as an output layer. The input size to the classification head should be the same as the output size of the fine-tuned BERT model, which can be obtained using the ``config.hidden_size`` attribute of the BERT configuration. The output size of the classification head should be equal to the number of output classes in our classification task. \n",
    "\n",
    "We will omit any further steps to the output of the final layer, as the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) is happy to receive the raw predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_dims: int, num_classes: int):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(embedding_dims, embedding_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dims, embedding_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dims, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a simple feedforward neural network with two hidden layers each followed by rectified linear activation and a dropout, followed by a linear layer with six output classes. Size and number of layers can be adjusted as needed, but this is a good starting point. Let's see the classification head in operation.\n",
    "\n",
    "The ``sequence_length`` is the static length of input sequences. The ``embedding_dims`` is set to the size of the BERT model's output embeddings, which is 768 for the ``bert-base-uncased`` model. In the original BERT paper they mention the following:\n",
    "\n",
    "_\"The first token of every sequence is always a special classification token (``[CLS]``). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\"_\n",
    "\n",
    "Consider a mini-batch of two tokenized sequences. The shape of the mini-batch tensor will be ``[2,512,768]``, where:\n",
    " - 2 corresponds to the number of sequences (i.e. the batch size)\n",
    " - 512 corresponds to the tokens in a sequence\n",
    " - 768 corresponds to the embedding dimensions for a token\n",
    "\n",
    "For such input, without any modifications the ``ClassificationHead`` should expect an input of ``[2,1,768]``. However, due to having everything encoded to a single token, the classifier can expected to receive a single token for each sentence in an input batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0197,  0.2046, -0.0770, -0.0518, -0.1724, -0.0071],\n",
       "        [-0.0590,  0.1225, -0.0316,  0.0219, -0.0804, -0.0677]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.rand(2, 1, 768)\n",
    "cls_head = ClassificationHead(embedding_dims=inputs.shape[2], num_classes=6)\n",
    "print(cls_head(inputs[:, 0, :]).shape)\n",
    "cls_head(inputs[:, 0, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining fine-tuned BERT with classification head\n",
    "\n",
    "Now that the classification head is defined, we can combine it with the fine-tuned BERT model using another ``nn.Sequential`` module. This combined model will take in the preprocessed input text data and output the predicted probability distribution over the output classes. We can then move the combined model to the device we plan to use for training and evaluation, such as a GPU if available.\n",
    "\n",
    "Here, pay attention to the `forward` method and its arguments. As discussed earlier with the dataset definition, the arguments conform to sample-wise keys of the dict provided by our dataset. These, in turn, conform to the data format expected by the ``BertForMaskedLM``.\n",
    "\n",
    "We also want to omit tuning our BERT models. That is achieved with the ``requires_grad = False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithSeparateClassificationHead(nn.Module):\n",
    "    def __init__(self, bert_mlm: BertForMaskedLM, num_classes: int):\n",
    "        super(BertWithSeparateClassificationHead, self).__init__()\n",
    "        self.bert_mlm = bert_mlm\n",
    "        for param in self.bert_mlm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = ClassificationHead(\n",
    "            embedding_dims=self.bert_mlm.config.hidden_size, num_classes=num_classes\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None, **kwargs):\n",
    "        outputs = self.bert_mlm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_hidden_state)\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a new class ``BertWithSeparateClassificationHead`` that inherits from ``nn.Module``. \n",
    "\n",
    "It uses a provided pre-trained BERT model and the previously defined ``ClassificationHead``. In the forward method, the input ``input_ids`` are first through the BERT model and the output of the ``[CLS]`` token from the last hidden state of the BERT model are extracted. Then, the ``[CLS]`` token embeddings are passed through the ``ClassificationHead`` and a softmax function to get the probabilities for the six output classes for each sentence in the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(1.7533, grad_fn=<NllLossBackward0>),\n",
       " 'logits': tensor([[-0.0703,  0.0388,  0.0642, -0.0541, -0.0276,  0.0426]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batchify\n",
    "batched_sample = {}\n",
    "for key, value in sample.items():\n",
    "    batched_sample[key] = value.unsqueeze(0)\n",
    "\n",
    "model_finetuned_with_classifier = BertWithSeparateClassificationHead(\n",
    "    bert_mlm=model_finetuned, num_classes=6\n",
    ")\n",
    "model_finetuned_with_classifier.to(\"cpu\")\n",
    "y_pred = model_finetuned_with_classifier.forward(**batched_sample)\n",
    "y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classification model\n",
    "\n",
    "To train the classification model for the medical specialties of the transcriptions, we will use the preprocessed data from the previous notebook, which includes a separate column for the medical specialty labels. We will use this column to obtain the labels for each transcription.\n",
    "\n",
    "To set up the training process, we use the Hugging Face ``Trainer`` class, as in the preceding notebook with the MLM task. During training, the model will learn to map the preprocessed input text data to the correct output class (i.e., medical specialty) using a combination of the fine-tuned BERT model and the custom classification head. We can monitor the training progress using various performance metrics, such as accuracy and loss, and make adjustments to the hyperparameters as needed to improve performance.\n",
    "\n",
    "We already know what to do from the fine-tuning process. Let's do it again, with necessary modifications. We'll define a function to train the composite model to help training both our fine-tuned and only pre-trained base BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "def train_bert_with_cls_head(model_name: str, bert_mlm: BertForMaskedLM) -> str:\n",
    "    trainer = Trainer(\n",
    "        model=BertWithSeparateClassificationHead(bert_mlm=bert_mlm, num_classes=6),\n",
    "        train_dataset=MedicalTranscriptionDataset(encodings=x_train, labels=y_train),\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"out\",\n",
    "            per_device_train_batch_size=24,\n",
    "            num_train_epochs=8,\n",
    "            dataloader_num_workers=8,\n",
    "        ),\n",
    "    )\n",
    "    trainer.train()\n",
    "    model_path = f\"../../model/{model_name}/model.pt\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    # We'll save like any Pytorch model, as the model is not a Hugging Face `PreTrainedModel`\n",
    "    torch.save(trainer.model, model_path)\n",
    "    return model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grimfada/miniconda3/envs/transformers-seminar-workshop/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2290\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 768\n",
      "  Number of trainable parameters = 1185798\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 12:12, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.258300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "fine_model_path = train_bert_with_cls_head(\n",
    "    model_name=\"classifier-fine\", bert_mlm=model_finetuned\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classification model\n",
    "\n",
    "To evaluate the performance of the classification model, we will use the preprocessed validation data from the previous notebook, which includes a separate column for the medical specialty labels. We will use this column to obtain the labels for each transcription.\n",
    "\n",
    "To evaluate the model, we can use the ``Trainer.evaluate()`` method to obtain the model's predictions on the validation data, and compare them to the true labels. We can compute various performance metrics, such as accuracy, precision, recall, and F1 score, to assess the model's performance on the classification task.\n",
    "\n",
    "Let's first define our metric computation function. It will receive as its input the prediction output of our model, see [Trainer.predict](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.predict) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = preds.predictions.argmax(-1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease up comparing models later, let's also define a function to load and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def load_and_evaluate_saved_model(model_path):\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer = Trainer(model=torch.load(model_path))\n",
    "    preds = trainer.predict(\n",
    "        test_dataset=MedicalTranscriptionDataset(\n",
    "            encodings=x_validation, labels=y_validation\n",
    "        )\n",
    "    )\n",
    "    return compute_metrics(preds=preds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then evaluate the composite model with a fine-tuned BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 572\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/72 00:21 < 00:00, 3.21 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grimfada/miniconda3/envs/transformers-seminar-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6083916083916084,\n",
       " 'precision': 0.5142903687221869,\n",
       " 'recall': 0.6083916083916084,\n",
       " 'f1': 0.5182635361941162}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_and_evaluate_saved_model(model_path=fine_model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our fine-tuned BERT with separate head to pre-trained BERT and separate head\n",
    "\n",
    "To compare the performance of our fine-tuned BERT model with separate head to a pre-trained BERT model with separate head, we can load the pre-trained BERT model using the Hugging Face ``BertForMaskedLM`` class, which includes a pre-trained BERT model. We will then join this non-fine-tuned pre-trained model with our own classification head.  We then train only the classification head for a similar amount of time as we did for our fine-tuned model.\n",
    "\n",
    "After training, we can evaluate the performance of both models on the same validation set and compare their performance metrics, such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "By comparing the performance of the fine-tuned BERT model with separate head to the pre-trained BERT model with separate head, we can assess the impact of our fine-tuning process on the performance of the model for the specific medical text classification task, as well as the benefit of using a custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/grimfada/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/grimfada/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/grimfada/miniconda3/envs/transformers-seminar-workshop/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2290\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 768\n",
      "  Number of trainable parameters = 1185798\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 12:18, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.351900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_pretrained = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "pre_model_path = train_bert_with_cls_head(\n",
    "    model_name=\"classifier-pre\", bert_mlm=model_pretrained\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 572\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/72 00:21 < 00:00, 3.20 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grimfada/miniconda3/envs/transformers-seminar-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6031468531468531,\n",
       " 'precision': 0.4926711462594029,\n",
       " 'recall': 0.6031468531468531,\n",
       " 'f1': 0.5013725085029777}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_and_evaluate_saved_model(model_path=pre_model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What can be deduced from the results? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this and preceding two notebooks we've gone through the process of preprocessing textual data for BERT-based NLP, fine-tuning a pre-trained model and creating a custom classification head for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-seminar-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
