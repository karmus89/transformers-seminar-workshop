{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the fine-tuned BERT model\n",
    "\n",
    "In the previous notebook, we walked through the process of fine-tuning a pre-trained BERT model using masked language modeling, and saved the fine-tuned model for later use. \n",
    "\n",
    "In this notebook, we will continue our exploration of BERT by using the fine-tuned model to perform classification on a medical text dataset. Specifically, we will define a custom classification head on top of the fine-tuned BERT model using PyTorch, and train and evaluate the classification model on a held-out test set of medical text data. \n",
    "\n",
    "We will also compare the performance of our fine-tuned model to an out-of-the-box BERT model, which has not been fine-tuned on the medical text dataset. By the end of this notebook, you will have a solid understanding of how to use a fine-tuned BERT model for classification of medical text data, and how to compare its performance to an out-of-the-box BERT model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "The notebook has been implemented to require the user to write their own code. Normal descriptive information is in plain text.\n",
    "\n",
    "> Any text within a quote block indicates instructions for the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the fine-tuned BERT model\n",
    "\n",
    "To use the fine-tuned BERT model for classification, we can load the saved model from the previous notebook using the ``BertForMaskedLM`` class from the Hugging Face Transformers library. This class is the same pre-trained BERT model that we fine-tuned using masked language modeling in the previous notebook, and does not include a classification head.\n",
    "\n",
    "To attach our own classification head to the fine-tuned BERT model, we can define a custom classification head using PyTorch, which we will do in the next section. Once the classification head is defined, we can combine it with the fine-tuned BERT model using the PyTorch ``nn.Sequential`` module. We can then move the combined model to the device we plan to use for training and evaluation, such as a GPU if available."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load a pre-trained ``bert-base-uncased`` tokenizer to ``tokenizer`` variable.\n",
    ">\n",
    "> Load the fine-tuned ``bert-base-uncased-finetuned`` from ``\"../../model\"`` to ``model`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load the processed data to ``df_train`` and ``df_validation``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dataset\n",
    "\n",
    "Before proceeding to training our model, we must first define a ``Dataset``. It needs to provide the model with inputs and target class label values. Let's begin with our inputs. We need to feed tokens to our model. Let's, therefore, transform our sentences to lists of tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tokenize training and validation transcriptions to ``x_train`` and ``x_validation`` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go for the target values. Let's check once more that what classes we have in our data. Those are the values of the ``\"medical_specialty\"`` column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extract unique medical specialtys from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our labels being in text, we need to map the labels to integers for our model. Let's create a label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "class_to_label = {\n",
    "    \"Surgery\": 0,\n",
    "    \"Radiology\": 1,\n",
    "    \"Consult\": 2,\n",
    "    \"Cardiovascular\": 3,\n",
    "    \"Orthopedic\": 4,\n",
    "    \"General Medicine\": 5,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then create a new ``\"label\"`` column to our datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map the contents of the ``medical_specialty`` column from string to integers for the training and validation data. Extract the mapped values as numpy arrays and persist them to ``y_train`` and ``y_test`` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then define our dataset. We'll leave our target values as indices, since the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) expects the labels as indices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a ``TranscriptionClassificationDataset`` that inherits from ``torch.utils.data.Dataset``. It should accept an input of the ``BatchEncoding`` type and integer labels for each input. Again, the dataset should output a dictionary with the respective tensors under each key: ``input_ids``, ``attention_mask``, ``token_type_ids`` and ``labels``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap here a bit. The thing to note here is that the output of our dataset must conform to what is expected by the BERT model. This needs to be taken into account especially when using the Hugging Face ``Trainer`` to train the model. From the documentation of the [BertFormMaskedLM.forward](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertForMaskedLM.forward) we can see that the model expects _at least_ the following arguments:\n",
    " - ``input_ids`` of shape ``[batch, sequence_length]`` (required)\n",
    " - ``attention`` of shape ``[batch, sequence_length]`` (optional)\n",
    " - ``token_type_ids`` of shape ``[batch, sequence_length]`` (optional)\n",
    " - ``labels`` of shape ``[batch, sequence_length]`` (optional)\n",
    "\n",
    "For the last one, ``labels``, the ``BertForMaskedLM`` model expects per-token labels but this time our labels are per-sentence. For sentence classification we will thus omit passing the per-sentence labels by design as our labels are per-sentence and not per-token. The per-sentence labels will be used with the separately defined classification head. For each batch of samples, the ``Trainer`` passes the data batched under each keyword as a separate keyword argument for the model's ``forward`` function. That is why the ``labels`` have still to be included in our samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then see what the ``TranscriptionClassificationDataset`` actually does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize a ``TranscriptionClassificationDataset`` and extract a sample. Inspect the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is important to notice that the dataset's samples must conform to the format expected by our underlying BERT model. \n",
    "\n",
    "Let's finally try to feed a sample to the model to see what it does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take a sample from the dataset and feed it to the initialized fine-tuned model. Remember, the fine-tuned model is an MLM model and expects the ``labels`` to be token-wise. We are about to perform sequence-wise classification, so ``labels`` should be omitted when passing data to the model. Examine the outputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the classification head\n",
    "\n",
    "To perform classification on the fine-tuned BERT model, we need to attach a custom classification head on top of the pre-trained model. This classification head will take the output embeddings from the fine-tuned BERT model and map them to a set of output classes using a feedforward neural network.\n",
    "\n",
    "To define the classification head, we can use PyTorch to create a new ``nn.Sequential`` module. The classification head should include at least one hidden layer with a ReLU activation function, as well as an output layer. The input size to the classification head should be the same as the output size of the fine-tuned BERT model, which can be obtained using the ``config.hidden_size`` attribute of the BERT configuration. The output size of the classification head should be equal to the number of output classes in our classification task. \n",
    "\n",
    "We will omit any further steps to the output of the final layer, as the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) is happy to receive the raw predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define a simple ``ClassificationHead`` that inherits from ``nn.Module`` and accepts an embedded sample as its input. The output should conform to the to vector having as many dimensions as there are classes in our classification task. The classification head should comprise of two hidden layers with ReLU activations and dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a simple feedforward neural network with two hidden layers each followed by rectified linear activation and a dropout, followed by a linear layer with six output classes. Size and number of layers can be adjusted as needed, but this is a good starting point. Let's see the classification head in operation.\n",
    "\n",
    "The ``embedding_dims`` is set to the size of the BERT model's output embeddings, which is 768 for the ``bert-base-uncased`` model. In the original BERT paper they mention the following: _\"The first token of every sequence is always a special classification token (``[CLS]``). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\"_ This is what the classification head expects as its input.\n",
    "\n",
    "Below is an example of how the classification head operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "import torch\n",
    "\n",
    "# BERT's example final output:\n",
    "# Batch of two samples, 10 tokens per sample and 768 embedding dimensions per token\n",
    "bert_output = torch.rand(2, 10, 768)\n",
    "print(f\"BERT example output: {bert_output.shape}\")\n",
    "\n",
    "# Classification head initialization\n",
    "cls_head = ClassificationHead(embedding_dims=bert_output.shape[2], num_classes=6)\n",
    "\n",
    "# Extract the first aggregate CLS-token for each sample in the batch\n",
    "cls_tokens = bert_output[:, 0, :]\n",
    "print(f\"CLS tokens: {cls_tokens.shape}\")\n",
    "\n",
    "# Feed the CLS tokens to the model\n",
    "output = cls_head(cls_tokens)\n",
    "\n",
    "print(f\"Classifier head output: {output.shape}\")\n",
    "output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining fine-tuned BERT with classification head\n",
    "\n",
    "Now that the classification head is defined, we can combine it with the fine-tuned BERT model using another ``nn.Sequential`` module. This combined model will take in the preprocessed input text data and output the predicted probability distribution over the output classes. We can then move the combined model to the device we plan to use for training and evaluation, such as a GPU if available.\n",
    "\n",
    "Here, pay attention to the `forward` method and its arguments. As discussed earlier with the dataset definition, the arguments conform to sample-wise keys of the dict provided by our dataset. These, in turn, conform to the data format expected by the ``BertForMaskedLM``.\n",
    "\n",
    "We also want to omit tuning our BERT models. That is achieved with the ``requires_grad = False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "class BertWithSeparateClassificationHead(nn.Module):\n",
    "    def __init__(self, bert_mlm: BertForMaskedLM, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_mlm = bert_mlm\n",
    "        self.classifier = ClassificationHead(\n",
    "            embedding_dims=self.bert_mlm.config.hidden_size, num_classes=num_classes\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Freeze the BERT MLM model\n",
    "        for param in self.bert_mlm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None, **kwargs):\n",
    "        # Feed the expected inputs to the BERT MLM model\n",
    "        bert_output = self.bert_mlm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        # Extract the last hidden state\n",
    "        last_hidden_state = bert_output.hidden_states[-1]\n",
    "\n",
    "        # Extract the aggregate CLS tokens of the last hidden state\n",
    "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Calculate classification probabilities\n",
    "        logits = self.classifier(cls_hidden_state)\n",
    "\n",
    "        # Return what is expected by the Trainer\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a new class ``BertWithSeparateClassificationHead`` that inherits from ``nn.Module``. \n",
    "\n",
    "It uses a provided pre-trained BERT model and the previously defined ``ClassificationHead``. In the forward method, the input ``input_ids`` are first through the BERT model and the output of the ``[CLS]`` token from the last hidden state of the BERT model are extracted. Then, the ``[CLS]`` token embeddings are passed through the ``ClassificationHead`` and a softmax function to get the probabilities for the six output classes for each sentence in the input batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take a sample from the dataset and feed it to the ``BertWithSeparateClassificationHead`` model. Examine the outputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classification model\n",
    "\n",
    "To train the classification model for the medical specialties of the transcriptions, we will use the preprocessed data from the previous notebook, which includes a separate column for the medical specialty labels. We will use this column to obtain the labels for each transcription.\n",
    "\n",
    "To set up the training process, we use the Hugging Face ``Trainer`` class, as in the preceding notebook with the MLM task. During training, the model will learn to map the preprocessed input text data to the correct output class (i.e., medical specialty) using a combination of the fine-tuned BERT model and the custom classification head. We can monitor the training progress using various performance metrics, such as accuracy and loss, and make adjustments to the hyperparameters as needed to improve performance.\n",
    "\n",
    "We already know what to do from the fine-tuning process. Let's do it again, with necessary modifications. We'll define a function to train the composite model to help training both our fine-tuned and only pre-trained base BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "def train_bert_with_cls_head(model_name: str, bert_mlm: BertForMaskedLM) -> str:\n",
    "    trainer = Trainer(\n",
    "        model=BertWithSeparateClassificationHead(bert_mlm=bert_mlm, num_classes=6),\n",
    "        train_dataset=TranscriptionClassificationDataset(inputs=x_train, labels=y_train),\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"out\",\n",
    "            per_device_train_batch_size=24,\n",
    "            num_train_epochs=8,\n",
    "            dataloader_num_workers=8,\n",
    "        ),\n",
    "    )\n",
    "    trainer.train()\n",
    "    model_path = f\"../../model/{model_name}/model.pt\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    # We'll save like any Pytorch model, as the model is not a Hugging Face `PreTrainedModel`\n",
    "    torch.save(trainer.model, model_path)\n",
    "    return model_path\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the classifier with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the classification model\n",
    "\n",
    "To evaluate the performance of the classification model, we will use the preprocessed validation data from the previous notebook, which includes a separate column for the medical specialty labels. We will use this column to obtain the labels for each transcription.\n",
    "\n",
    "To evaluate the model, we can use the ``Trainer.evaluate()`` method to obtain the model's predictions on the validation data, and compare them to the true labels. We can compute various performance metrics, such as accuracy, precision, recall, and F1 score, to assess the model's performance on the classification task.\n",
    "\n",
    "Let's first define our metric computation function. It will receive as its input the prediction output of our model, see [Trainer.predict](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.predict) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = preds.predictions.argmax(-1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease up comparing models later, let's also define a function to load and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "def load_and_evaluate_saved_model(model_path):\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer = Trainer(model=torch.load(model_path))\n",
    "    preds = trainer.predict(\n",
    "        test_dataset=TranscriptionClassificationDataset(\n",
    "            inputs=x_validation, labels=y_validation\n",
    "        )\n",
    "    )\n",
    "    return compute_metrics(preds=preds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then evaluate the composite model with a fine-tuned BERT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate fine-tuned classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our fine-tuned BERT with separate head to pre-trained BERT and separate head\n",
    "\n",
    "To compare the performance of our fine-tuned BERT model with separate head to a pre-trained BERT model with separate head, we can load the pre-trained BERT model using the Hugging Face ``BertForMaskedLM`` class, which includes a pre-trained BERT model. We will then join this non-fine-tuned pre-trained model with our own classification head.  We then train only the classification head for a similar amount of time as we did for our fine-tuned model.\n",
    "\n",
    "After training, we can evaluate the performance of both models on the same validation set and compare their performance metrics, such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "By comparing the performance of the fine-tuned BERT model with separate head to the pre-trained BERT model with separate head, we can assess the impact of our fine-tuning process on the performance of the model for the specific medical text classification task, as well as the benefit of using a custom classification head."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load the pre-trained ``bert-base-uncased`` model and train a new classifier using it as the BERT MLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate the pre-trained only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What can be deduced from the results? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this and preceding two notebooks we've gone through the process of preprocessing textual data for BERT-based NLP, fine-tuning a pre-trained model and creating a custom classification head for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-seminar-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
