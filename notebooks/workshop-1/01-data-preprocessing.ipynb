{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Natural Language Processing (NLP) is a rapidly growing field that has numerous applications in various domains, including healthcare. The medical field has an abundance of data that can be utilized for NLP tasks, such as medical transcriptions and clinical notes. However, this data often requires extensive preprocessing before it can be used effectively for machine learning models.\n",
    "\n",
    "The dataset we'll be using comes from the [socd6/medical-nlp](https://github.com/socd06/medical-nlp0) repository.\n",
    "This dataset, compiled for NLP experiments, contains medical transcriptions and custom-generated clinical stop words and vocabulary. It includes a fully processed dataset simplified to 4 classes: Surgery, Medical Records, Internal Medicine, and Other. The dataset is divided into training and test subsets to allow for the development and evaluation of machine learning models. The dataset also contains the original medical specialties for each transcription."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this workshop is to finetune a BERT (Bidirectional Encoder Representations from Transformers) model to classify medical transcriptions. BERT is a powerful NLP model that has achieved state-of-the-art results on a wide range of NLP tasks. By using BERT, we aim to improve the accuracy and effectiveness of our classification model.\n",
    "\n",
    "In this Jupyter Notebook, we will explore the preprocessing steps required to prepare the dataset for use in the BERT model. We will generate two datasets:\n",
    "\n",
    " 1. Fine-tuning dataset to have the BERT model better suit handling medical transcriptions.\n",
    " 2. Validation dataset to inspect the classification accuracy of the BERT models.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "The notebook has been implemented to require the user to write their own code. Normal descriptive information is in plain text.\n",
    "\n",
    "> Any text within a quote block indicates instructions for the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial load\n",
    "\n",
    "Let's first begin by taking a look at our data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Use the pandas library to read in the medical-nlp dataset from the file path ``\"../../data/medical-nlp/mtsamples.csv\"`` and assign it to a variable named ``df``.\n",
    ">\n",
    "> Print out a list of the columns in the dataset.\n",
    ">\n",
    "> Display the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also review column-wise contents.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate a summary of descriptive statistics for the numeric columns in the dataset\n",
    ">\n",
    "> Display the resulting summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data handling\n",
    "\n",
    "It seems that not every row have all the values. Let's investigate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Check the dataset for missing values.\n",
    ">\n",
    ">Display the number of missing values in each column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical appointment transcriptions are full length texts that we are really interested in. Let's drop any rows that are missing the values for `transcription`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Remove any rows from the ``df`` dataset where the ``transcription`` column contains missing values.\n",
    ">\n",
    ">Display a summary of descriptive statistics for the numeric columns in the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "assert df.shape == (4966, 5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "In this section, we will explore the ``medical_specialty`` and ``transcription`` columns of the medical-nlp dataset in more detail. These columns contain valuable information about the medical specialties and corresponding transcriptions of the medical reports.\n",
    "\n",
    "The ``medical_specialty`` column indicates the medical specialty associated with each report, such as cardiology, neurology, or orthopedics. This column may provide useful insights into the distribution of medical reports across different specialties and the potential differences in language use between them.\n",
    "\n",
    "The ``transcription`` column contains the text of the medical reports themselves. This column is of particular interest as it contains the primary data we will be using to train and evaluate our machine learning models. The text data in this column will require extensive preprocessing to ensure that it is in a suitable format for use with our models.\n",
    "\n",
    "In the following cells, we will explore the distribution of medical specialties in the dataset and examine some example transcriptions to gain a better understanding of the text data we are working with. We will also discuss the preprocessing steps required to prepare the text data for use with machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `medical_specialty`\n",
    "\n",
    "Let's begin with the `medical_specialty` column. First we find out the unique values in the column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Retrieve the unique values from the ``medical_specialty`` column of the ``df`` dataset.\n",
    ">\n",
    ">Print out the number of unique medical specialties in the dataset.\n",
    ">\n",
    ">Display a list of the unique medical specialties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then visualize the distribution of medical specialties in our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Count the number of occurrences of each medical specialty in the ``medical_specialty`` column of the ``df`` dataset.\n",
    ">\n",
    ">Generate a horizontal bar chart to visualize the distribution of medical specialties.\n",
    ">\n",
    ">Set the size of the chart to fit the image within the Jupyter Notebook cell and adjust the font size as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the dataset is skewed rather than uniformly distributed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `transcription`\n",
    "\n",
    "Let's then find some descriptive details about the transcriptions too. Let's begin by getting the distribution of transcription character lengths."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Retrieve the length of each ``transcription`` in the transcription column of the ``df`` dataset.\n",
    ">\n",
    ">Plot an empirical cumulative distribution function (ECDF) of the transcription lengths using Seaborn's ``ecdfplot()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median length of the transcriptions is around 3000 chracters. This doesn't, however, tell a lot as with NLP single characters are not that telling enough."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the texts\n",
    "\n",
    "Here's an outline of the steps you can take to prepare your data for an NLP task. These preprocessing steps are not always necessary or appropriate for every NLP task or dataset. However, they are commonly applied to text data to help improve the accuracy and efficiency of NLP models:\n",
    "\n",
    "- __Lowercasing the text__: Text data is often case sensitive, meaning that \"hello\" and \"Hello\" would be treated as different words by NLP models. Lowercasing the text can help to standardize the text and reduce the dimensionality of the feature space.\n",
    "\n",
    "- __Removing punctuation__: Punctuation marks such as commas, periods, and question marks do not contribute much to the meaning of text and can add noise to NLP models. Removing punctuation can help to simplify the text and make it easier for models to learn from.\n",
    "\n",
    "- __Tokenization__: Breaking text data into individual words or subwords (tokens) is a fundamental step in many NLP tasks. Tokenization helps to simplify the text and make it easier to analyze, process, and understand.\n",
    "\n",
    "- __Removing stop words__: Stop words are common words such as \"the\", \"and\", and \"a\" that do not carry much meaning in text data. Removing stop words can help to simplify the text and reduce the dimensionality of the feature space.\n",
    "\n",
    "- __Stemming or lemmatization__: Words in text data can have different forms (e.g., \"run\", \"running\", \"ran\"). Stemming or lemmatization is the process of reducing words to their base form, such as \"run\". This can help to standardize the text and reduce the dimensionality of the feature space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "What we're interested in are the __tokens__ of the transcription. In NLP, a token is a sequence of characters that represents a unit of meaning. It is typically a word, but can also be a phrase or a subword. The process of breaking text into tokens is called tokenization, and it is a fundamental step in many NLP tasks, including text classification, machine translation, and sentiment analysis.\n",
    "\n",
    "Tokenization is necessary because most NLP algorithms and models operate on discrete units of text, rather than on continuous text. By breaking text into tokens, we can analyze, process, and understand the meaning of text more efficiently and effectively.\n",
    "\n",
    "A token can be defined in many ways, depending on the specific NLP task and the level of granularity required. For example, a token can be a sequence of characters separated by white spaces, a sequence of characters separated by punctuation marks, or a subword obtained through a more complex process such as byte-pair encoding or wordpiece.\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    "        \"a fox was happily jumping in the field with a hedgehog\"\n",
    "\n",
    "To tokenize the text, it can be  split into individual words, resulting in the following list of tokens:\n",
    "\n",
    "        [\"a\", \"fox\", \"was\", \"happily\", \"jumping\", \"in\", \"the\", \"field\", \"with\", \"a\", \"hedgehog\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Stop word removal is a preprocessing step in NLP that involves removing common words that do not carry much meaning in text data. These common words are known as stop words and typically include words such as \"the\", \"and\", \"a\", and \"in\".\n",
    "\n",
    "The goal of stop word removal is to simplify the text and reduce the dimensionality of the feature space, which can improve the efficiency and accuracy of NLP models. Removing stop words can also help to reduce noise in the text and improve the interpretability of the results.\n",
    "\n",
    "However, stop word removal should be used with care, as removing too many words can lead to loss of information and potential reduction in model performance. Additionally, the list of stop words used may vary depending on the language of the text, the specific NLP task at hand, and the requirements of the downstream NLP model.\n",
    "\n",
    "For our tokenized sentence, the stop words could very well be `\"a\"`, `\"was\"` and `\"the\"`. After the removal, the tokenized sentence would turn out to be:\n",
    "\n",
    "        [\"fox\", \"happily\", \"jumping\", \"in\", \"field\", \"with\", \"hedgehog\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming/Lemmatization\n",
    "\n",
    "Stemming (or interchangeably lemmatization) is a process in NLP that involves reducing words to their base form, or stem. The goal of stemming is to standardize the text and reduce the dimensionality of the feature space, which can help improve the accuracy and efficiency of NLP models.\n",
    "\n",
    "There are several algorithms that can be used for stemming, including the Porter stemming algorithm, the Snowball stemming algorithm, and the Lancaster stemming algorithm. These algorithms apply a set of rules to reduce words to their base form, such as removing suffixes or prefixes, and may use language-specific rules or heuristics to handle irregular words.\n",
    "\n",
    "For example, the Porter stemming algorithm first breaks the sentence into individual words, based on the spaces between the words. In this case, the resulting words would be:\n",
    "\n",
    "        \"fox\" would become \"fox\"\n",
    "        \"happily\" would become \"happili\"\n",
    "        \"jumping\" would become \"jump\"\n",
    "        \"in\" would become \"in\"\n",
    "        \"field\" would become \"field\"\n",
    "        \"with\" would become \"with\"\n",
    "        \"hedgehog\" would become \"hedgehog\"\n",
    "\n",
    "This would turn the tokens in the initial sentence into:\n",
    "\n",
    "        [\"fox\", \"happili\", \"jump\", \"in\", \"field\", \"with\", \"hedgehog\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection\n",
    "\n",
    "Let's first begin by selecting only the data that we're interested in. Let's select only those medical specialties that have more than 5 % coverage in the whole dataset to have at least somewhat balanced dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Filter the dataframe to include only the rows corresponding to medical specialties that are represented by more than 5 % of the data, and store the resulting filtered dataframe in a new variable `df_processed`.\n",
    ">\n",
    ">Compute and print the frequency of each medical specialty in the filtered dataframe as a proportion of the total number of rows.\n",
    ">\n",
    ">Create a horizontal bar chart to visualize the frequency distribution of medical specialties in the filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then select the columns that are of interest for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Select the ``medical_specialty`` and ``transcription`` columns from the processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "assert df_processed.shape == (2862, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also clean up the medical specialtys. The specialtys seem to have a trailing whitespace. Let's clean the column up a bit. Some specialtys are also multipart. For brevity, let's take only the first part of the specialty. We can always come back to see the whole specialty if need be."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split the medical specialty values with `\"/\"` and `\"-\"` and select only the first part of the specialty.\n",
    ">\n",
    "> Trim trailing whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "assert any([\"Pulmonary\" not in c for c in df_processed[\"medical_specialty\"].unique()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Let's perform the preprocessing in one go. The ``preprocess_text`` function is a text preprocessing function in NLP that takes as input a string of text and applies several preprocessing steps in sequence to clean and transform the text for use in downstream NLP tasks.\n",
    "\n",
    "The preprocessing steps applied in this function include:\n",
    "1. Lowercasing the text to standardize the text and reduce the dimensionality of the feature space.\n",
    "1. Removing punctuation from the text to remove noise and simplify the text.\n",
    "1. Tokenizing the text to break it into individual words, which can be used as features in downstream NLP models.\n",
    "1. Removing stop words from the text, which are common words that do not carry much meaning in text data, to reduce noise and improve the efficiency and accuracy of downstream NLP models.\n",
    "1. Stemming/Lemmatizing the words in the text, which involves reducing words to their base form to further reduce the dimensionality of the feature space and improve the accuracy of downstream NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace punctuation with spaces\n",
    "    text = re.sub(rf\"[{string.punctuation}]\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Return the preprocessed text as a string\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "preprocess_text(\"This is an example,.that kind of looks like, , 1.0,the data in question\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then preprocess our data with the function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Apply a text preprocessing function to a column of text data in a Pandas dataframe, such as the \"transcription\" column, and store the preprocessed text in a new column of the same dataframe.\n",
    ">\n",
    ">Inspect the first few rows of a Pandas dataframe containing preprocessed text data, to verify that the preprocessing function was applied correctly and that the resulting text data is in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "assert isinstance(df_processed.iloc[0,1], str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the character level lengths of processed transcriptions for comparison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Retrieve the length of each ``transcription`` in the transcription column of the ``df_processed`` dataset.\n",
    ">\n",
    ">Plot an empirical cumulative distribution function (ECDF) of the transcription lengths using Seaborn's ``ecdfplot()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median length is now around 2000 characters and the longer transcriptions are also considerably shorter now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "The last part is to split the processed data into fine-tuning and validation datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split the processed data into 80 % training (``df_train``) and 20 % validation (`df_validation`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "assert 2280 < df_train.shape[0] < 2300\n",
    "assert 560 < df_validation.shape[0] < 580"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data\n",
    "\n",
    "The last step in preprocessing is saving the processed training and validation datasets for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "import os\n",
    "\n",
    "for data in [\"train\", \"validation\"]:\n",
    "    path = f\"../../data/{data}.csv\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Persist the training and validation sets with indices as CSVs to the repository's data-folder:\n",
    "> - Training CSV path: `\"../../data/train.csv\"`\n",
    "> - Validation CSV path: `\"../../data/validation.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "assert os.path.isfile(\"../../data/train.csv\")\n",
    "assert os.path.isfile(\"../../data/validation.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
