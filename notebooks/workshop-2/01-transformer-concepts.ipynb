{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer core concepts\n",
    "\n",
    "This Jupyter notebook explores the core concepts of the Transformer model, a powerful architecture widely used in natural language processing (NLP) tasks. The notebook covers the following concepts:\n",
    "\n",
    "1. __Matrix multiplication__: A refresher on matrix multiplication, which is a fundamental operation used in the Transformer model.\n",
    "1. __Embedding vectors__: An explanation of how embedding vectors are used to represent words or subwords as numerical vectors, capturing their meaning and relationships.\n",
    "1. __Positional encoding__: An introduction to positional encoding, a technique used in the Transformer model to inject information about word order or position into the embedding vectors.\n",
    "1. __Self-attention__: An exploration of self-attention, a mechanism in the Transformer model that allows the model to weigh the importance of different parts of the input and capture relationships between elements.\n",
    "1. __Basic self-attention__: A demonstration of basic self-attention in PyTorch, showcasing how input tokens are weighted and combined to produce attention-weighted outputs.\n",
    "1. __Scaling the dot product__: An explanation of the scaling applied to the dot product in self-attention to prevent the inputs to the softmax function from growing too large.\n",
    "1. __Queries, keys, and values__: An introduction to the query, key, and value components in self-attention, which enable the model to focus on relevant parts of the input and compute attention scores.\n",
    "1. __Multi-head attention__: An exploration of multi-head attention, where multiple self-attention mechanisms, known as attention heads, are combined to enhance the model's ability to capture different relationships between tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By completing this notebook, readers will:\n",
    "\n",
    "1. Understand the core concepts of the Transformer model, including matrix multiplication, embedding vectors, positional encoding, self-attention, and multi-head attention.\n",
    "1. Gain insights into how self-attention mechanisms work and their role in capturing relationships between input elements.\n",
    "1. Learn how to implement basic self-attention in PyTorch and visualize the weighting and combination of input tokens.\n",
    "1. Comprehend the importance of scaling the dot product in self-attention to prevent large inputs to the softmax function.\n",
    "1. Gain familiarity with queries, keys, and values in self-attention and their roles in attention score computation.\n",
    "1. Understand the benefits of multi-head attention in capturing different relationships between tokens and enhancing the model's performance.\n",
    "\n",
    "With this knowledge, readers will be well-equipped to dive deeper into the Transformer model and apply it to various NLP tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher: Matrix multiplication\n",
    "\n",
    "Before we continue, however, it is a good idea to take a little refresher on _matrix multiplication_ ($*$). Let's take matrices $M$ and $N$ as an example. $M$ has 2 rows ($i$) and 4 columns ($j$), while $N$ has 4 rows and 2 columns. \n",
    "\n",
    "To multiply these matrices, we start by taking the dot product ($\\times$) of the first row of $M$ with the first column of $N$. To do this, we multiply the first element in the first row of $M$ with the first element in the first column of $N$, then add the result to the product of the second element in the first row of $M$ and the second element in the first column of $N$, and so on for all four elements in the row and column.\n",
    "\n",
    "We repeat this process for all rows in $M$ and all columns in $N$, and write the results into a new matrix that has the same number of rows as $M$ and the same number of columns as $N$. In this example, the resulting matrix would have 2 rows and 2 columns.\n",
    "\n",
    "With matrix multiplication, it is necessary to have the shapes conform to a simple principle where the the number of columns of the left-side matrix matches the number to rows in the right-side matrix. The resulting matrix will then have the row-count of the left-side matrix and the column-count of the right-side matrix. Let's define two matrices:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "M &= 2 \\times 4 \\enskip \\text{matrix} \\\\\n",
    "N &= 4 \\times 2 \\enskip \\text{matrix} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "To multiply the matrices we need to first check if $M_\\text{columns} = N_\\text{rows} \\to 4 = 4$. The result of the matrix multiplication will then have the shape of $M_\\text{rows} \\times N_\\text{cols} \\to 2 \\times 2$. The above process is depicted below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](../../diagrams/matrix-multiplication.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the dot product is the sum of products of values in two same-sized vectors and the matrix multiplication is a matrix version of the dot product with two matrices. The output of the dot product is a scalar whereas that of the matrix multiplication is a matrix whose elements are the dot products of pairs of vectors in each matrix.\n",
    "\n",
    "Keeping this in mind is important when dealing with data fed to neural networks. The data is predominantly multi-dimensional tensor data. Think of a matrix with an arbitrary number of dimensions, such as a 3-dimensional or even a 64-dimensional matrix. With NLP, the data has usually four dimensions:\n",
    " - the batch size\n",
    " - number of input sequences per batch\n",
    " - the number of tokens per sequence\n",
    " - the number of embedding values per token\n",
    "\n",
    "Aligning the tensors requires extra attention even when just combining pre-trained models to a simple classification head - the outputs of preceding steps must match the expected input shapes or error abound.\n",
    "\n",
    "A note of caution, though. When looking at how these are implemented in popular Python libraries, such as `numpy` or `torch`, the naming conventions seem to overlap. Let's look at `numpy` as an example. First, we initialize two differently shaped matrices, `a` and `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "[[1 2 3]\n",
      " [1 2 3]]\n",
      "b=\n",
      "[[0]\n",
      " [1]\n",
      " [2]]\n",
      "{'a.shape': (2, 3), 'b.shape': (3, 1)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3],[1,2,3]])\n",
    "b = np.array([[0],[1],[2]])\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print({\n",
    "    \"a.shape\":a.shape,\n",
    "    \"b.shape\":b.shape\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy offers us several ways to multiply these matrices out-of-the-box. The first is the `dot` function, which produces a dot product of two _arrays_. In NumPy, arrays can have arbitrary number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab=\n",
      "[[8]\n",
      " [8]]\n",
      "ab.shape=\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "ab = np.dot(a,b)\n",
    "print(\"ab=\")\n",
    "print(ab)\n",
    "print(\"ab.shape=\")\n",
    "print(ab.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there is the `matmul` function, which produces the matrix multiplication of two _arrays_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab=\n",
      "[[8]\n",
      " [8]]\n",
      "ab.shape=\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "ab = np.matmul(a,b)\n",
    "print(\"ab=\")\n",
    "print(ab)\n",
    "print(\"ab.shape=\")\n",
    "print(ab.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the `multiply` function, which then produces element-wise multiplication. The difference to the two above is that the matrices need to be equal in shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply(a,a)=\n",
      "[[1 4 9]\n",
      " [1 4 9]]\n",
      "multiply(b,b)=\n",
      "[[0]\n",
      " [1]\n",
      " [4]]\n",
      "multiply(a,b)=\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmultiply(b,b))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmultiply(a,b)=\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39;49mmultiply(a,b))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (3,1) "
     ]
    }
   ],
   "source": [
    "print('multiply(a,a)=')\n",
    "print(np.multiply(a,a))\n",
    "print('multiply(b,b)=')\n",
    "print(np.multiply(b,b))\n",
    "print('multiply(a,b)=')\n",
    "print(np.multiply(a,b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, PyTorch normally operates with batched matrix multiplication, where the first dimensions is treated as batch size and is, thus, omitted from the multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab=\n",
      "tensor([[[8],\n",
      "         [8]]])\n",
      "ab_tensor.shape=\n",
      "torch.Size([1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create tensors and add one dimension to simulate batch size\n",
    "a_tensor = torch.tensor(a).unsqueeze(0)\n",
    "b_tensor = torch.tensor(b).unsqueeze(0)\n",
    "ab_tensor = torch.bmm(a_tensor,b_tensor)\n",
    "print(\"ab=\")\n",
    "print(ab_tensor)\n",
    "print(\"ab_tensor.shape=\")\n",
    "print(ab_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For additional reading, see the following resources:\n",
    "- [What Are Dot Product and Matrix Multiplication?](https://mkang32.github.io/python/2020/08/23/dot-product.html)\n",
    "- [What Should I Use for Dot Product and Matrix Multiplication?](https://mkang32.github.io/python/2020/08/30/numpy-matmul.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding vectors\n",
    "\n",
    "The intuition behind embedding tokens to a vector is to represent each word or subword in a text as a numerical vector that captures its meaning and relationship to other words in the text. This is the basic idea behind word embeddings and subword embeddings, which are widely used in natural language processing (NLP) tasks.\n",
    "\n",
    "The key intuition behind embeddings is that words or subwords that have similar meanings or are used in similar contexts should be represented by similar or nearby embedding vectors in the high-dimensional space. This allows the embedding vectors to capture semantic and syntactic relationships between words, such as synonyms, antonyms, and analogies. \n",
    "\n",
    "For example, the embedding vectors for \"dog\" and \"cat\" might be closer together than the embedding vectors for \"dog\" and \"car\", reflecting their semantic similarity. Let's demonstrate this with a code experiment. In the following code cell:\n",
    " - The embedding vectors are defined as a 2D array called ``embeddings``, where each row represents the embedding vector for a word. For example, the embedding vector for \"cat\" is ``[0.3, 0.2, -0.1]``, while the embedding vector for \"dog\" is ``[0.1, 0.4, 0.2]``, and so on. Vectors contain only three values so that they can be easily plotted in 3D.\n",
    " - By visualizing the embedding vectors in 3D space, we can gain insights into how similar or different words are in terms of their embeddings. Words with similar meanings or used in similar contexts should have embedding vectors that are closer together in the high-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define some example vocabulary and corresponding embedding vectors\n",
    "vocab = [\"cat\", \"dog\", \"car\", \"tree\"]\n",
    "embeddings = np.array(\n",
    "    [[0.3, 0.2, -0.1], [0.1, 0.4, 0.2], [-0.2, 0.1, 0.4], [0.1, -0.1, 0.3]]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to plot the embeddings in 3D space\n",
    "def plot_embeddings_3d(vocab, embeddings):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(\n",
    "        111,\n",
    "        projection=\"3d\",\n",
    "    )\n",
    "    ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2])\n",
    "    for i, word in enumerate(vocab):\n",
    "        ax.text(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], word)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the embeddings in 3D space\n",
    "plot_embeddings_3d(vocab, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings can be learned from scratch on a specific task or dataset, or they can be pre-trained on large amounts of unlabeled data and then fine-tuned on smaller labeled datasets for specific tasks. Pre-trained embeddings, such as those used in BERT, have been shown to be highly effective for a wide range of NLP tasks and are widely used in the field. \n",
    "\n",
    "What is most important is that with embeddings text can be given to neural network in numerical format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "In this part, we will introduce the concept of positional encoding as it has been defined in the original Transformers paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "The intuition behind positional encoding is to inject some notion of word order or position into the embedding vectors, which is necessary for sequence-to-sequence models like transformers that process variable-length input sequences.\n",
    "\n",
    "In natural language processing (NLP), word order is critical to understanding the meaning of a sentence or paragraph. However, traditional word embeddings like word2vec or GloVe do not capture any information about word order or position in the input sequence. This means that a traditional embedding alone would not be sufficient for a model like a transformer to fully understand a given text.\n",
    "\n",
    "Positional encoding addresses this limitation by adding an additional vector to the word embeddings that encodes the position of each word in the sequence. This vector is added to the input embeddings before they are passed through the transformer layers, allowing the model to better understand the relationships between words based on their position in the sequence.\n",
    "\n",
    "The formula for computing the positional encoding vector for a word at a given position $\\text{pos}$ and dimension $i$ is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "PE_{pos,2i} & = \\text{sin} \\left( \\frac{pos}{10000^{2i/d_{embed}}} \\right)\\text{,  if }i\\text{ is even} \\\\\n",
    "PE_{pos,2i+11} & = \\text{cos} \\left( \\frac{pos}{10000^{2i/d_{embed}}} \\right)\\text{,  if }i\\text{ is odd} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Here, $\\text{pos}$ is the position of the word in the sequence, $i$ is the dimension of the positional encoding vector, and $d_{embed}$ is the dimension of the input embeddings. The value 10000 is a hyperparameter that determines the scale of the sine and cosine functions. \n",
    "\n",
    "Let's use an example. Let's assume that the word ``hello`` maps to an embedding vector ``[0.1, 0.2]``. Below is in an illustration how applying positional encoding to the vector in different changes the original embedding values. In other words, the the sentence we're using is\n",
    "\n",
    "        [\"hello\", \"hello\", \"hello\", \"hello\"]\n",
    "\n",
    "or\n",
    "\n",
    "        [[0.1, 0.2], [0.1, 0.2], [0.1, 0.2], [0.1, 0.2]]\n",
    "\n",
    "The code example provides a hands-on demonstration of how positional encoding can capture the position-related information in the embedding vectors, enabling transformer models to better understand the relationships between words based on their positions in a sequence. In the example:\n",
    " - We define a function called ``positional_encoding`` that takes a position pos and the dimension of the embedding vector ``embedding_dim`` as input. It calculates the positional encoding vector for the given position using a sine and cosine function based on the provided formulas.\n",
    " - Using an example embedding vector ``[0.1, 0.2]`` we create an array of embeddings where each row represents the same embedding vector. Then we iterate over each position in the sequence and calculate the corresponding positional encoding using the ``positional_encoding`` function. The resulting positional encodings are stored in the ``pos_encodings`` matrix.\n",
    " - Finally, we visualize the positional embeddings in a scatter plot. Each embedding vector is represented as a point in the plot, and the positions are indicated by different colors and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def positional_encoding(pos, embedding_dim):\n",
    "    pe = np.zeros((embedding_dim,))\n",
    "    for i in range(0, embedding_dim, 2):\n",
    "        pe[i] = np.sin(pos / (10000 ** (i / embedding_dim)))\n",
    "        pe[i + 1] = np.cos(pos / (10000 ** ((i + 1) / embedding_dim)))\n",
    "    return pe\n",
    "\n",
    "\n",
    "embedding = [0.1, 0.2]\n",
    "embeddings = np.array([embedding, embedding, embedding, embedding])\n",
    "\n",
    "pos_encodings = np.zeros((len(embeddings), len(embedding)))\n",
    "for pos in range(len(embeddings)):\n",
    "    pos_encodings[pos] = positional_encoding(pos, len(embedding))\n",
    "\n",
    "pos_embed = embeddings + pos_encodings\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "for i in range(len(embeddings)):\n",
    "    ax.scatter(pos_embed[i, 0], pos_embed[i, 1], label=f\"Position {i}\")\n",
    "ax.set_title(\"Positional Embedding:\\n Same token, different positions  \")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the scatter plot, we can see how the positional encodings modify the original embedding values for the same token. As the position of the token changes, the resulting embedding vectors also change, reflecting the influence of position on the embedding representation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For additional reading, please see the following resources:\n",
    "- [The Annotated Transformer: Positional Encoding](http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self-attention mechanism is like a teacher who helps you focus on what's important in a classroom. In the same way, the self-attention mechanism helps the transformer focus on which parts of the input it should be paying attention to, when processing information. This is achieved by allowing the model to weigh the importance of different parts of the input, giving more emphasis to the parts that are more relevant to the output.\n",
    "\n",
    "The weighing of inputs is done by computing **attention scores** between input elements (e.g. words in a sentence) and then using these scores to compute a **weighted sum of the input elements**. The attention **scores are computed using a learned function (_neural network_)** that takes as input both the current input element and a \"query\" vector that represents the current state of the model. The resulting **weighted sum is then used as input** to the next layer of the transformer. \n",
    "\n",
    "Below is an illustration of how an input sequence $x$ is weighted when fed to a transformer's $TF$ first layer $TF_0$.\n",
    "\n",
    "![image](../../diagrams/attention.png)\n",
    "\n",
    "The self-attention mechanism in a transformer-based architecture allows the model to analyze all tokens in an input sequence simultaneously, unlike recurrent neural networks (RNNs) that process tokens sequentially. In RNNs, the model passes information from one token to the next through a hidden state, making it difficult to parallelize computation across the sequence. \n",
    "\n",
    "In contrast, transformers apply an self-attention mechanism that allows the model to look at all tokens in the sequence at once, and to dynamically weigh the importance of each token based on its relationship to the other tokens in the sequence. This parallelization across the sequence enables transformer models to process longer sequences more efficiently than RNNs.\n",
    "\n",
    "Next we will go through the following topics:\n",
    " - __Basic self-attention__: The building block of attention mechanisms\n",
    " - __Scaling the dot product__: Ensuring that the computation are stable\n",
    " - __Queries, Keys and Values__: From non-parameterized self-attention to learned self-attention\n",
    " - __Multi-head attention__: Learning multiple ways to determine important parts in input sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic self-attention\n",
    "\n",
    "The basic self-attention serves as a building block for creating a complete attention mechanism in a transformer model. The basic self-attention operates on sequence level by weighing tokens based on the context of the sequence only (i.e. no learning).\n",
    "\n",
    "> Note: In the original transformer model, the attention mechanism enables the model to focus on different parts of the input sequence and capture the dependencies or connections between tokens effectively. The attention mechanism is used in two key components of the transformer: the encoder and the decoder. The basic self-attention is only a part of the whole attention mechanism operating on sequences.\n",
    "\n",
    "Let's try to understand this fundamental building block by implementing the basic self-attention operation in Python. The first thing we should do is work out how to express the self attention in matrix multiplications. \n",
    "\n",
    "There are no parameters in basic self-attention (yet). What the basic self-attention actually does is entirely determined by whatever mechanism creates the input sequence. Upstream mechanisms, like an embedding layer, drive the self-attention by learning representations with particular dot products (although we’ll add a few parameters later).\n",
    "\n",
    "For an input sequence $x$, basic self-attention for each token $x_i$ can be defined as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "w'_i & = x_i^Tx_i \\\\\n",
    "w_i & = \\text{softmax}(w'_i) \\\\\n",
    "y_i & =  w_ix_i \\\\\n",
    "\\end{aligned}$$\n",
    "where $w'_i$ are the raw weights, $w_i$ the scaled weights and $y_i$ the output of the self-attention mechanism. The shape of the $w'_i$ depends on the number of tokens in the input sequence $x$ and the number of embedding dimensions in each token $x_i$.\n",
    "\n",
    "Using the above definitions, the illustrated process of perorming basic self-attention on an input is depicted below. Let's define $x$ as an input with two tokens, each being a four-dimensional embedding vector. First the raw weights $w'_i$ are calculated:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](../../diagrams/basic-self-attention-raw-weight.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 22],\n",
       "       [22, 50]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2,1,2],[3,4,3,4]])\n",
    "w = np.dot(x,x.T)\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the raw weights are scaled:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../../diagrams/basic-self-attention-weight.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24835426e-18, 6.91440011e-13],\n",
       "       [6.91440011e-13, 1.00000000e+00]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "w = softmax(w)\n",
    "w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the input is weighted, producing the self-attention output:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../../diagrams/basic-self-attention-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.07432428e-12, 2.76576854e-12, 2.07432428e-12, 2.76576854e-12],\n",
       "       [3.00000000e+00, 4.00000000e+00, 3.00000000e+00, 4.00000000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(w,x)\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the basic self-attention would diminish the importance of the first token, leaving the second token to contribute to later stages of the computations. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For additional reading, see the following resources:\n",
    " - [Basics of Self-Attention](https://towardsdatascience.com/self-attention-5b95ea164f61)\n",
    " - [Transformers from scratch: Self attention](https://peterbloem.nl/blog/transformers#self-attention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries, keys and values\n",
    "\n",
    "The query, key, and value-based attention mechanism is an extension and development of the basic self-attention mechanism. While the basic self-attention treats each input token equally and operates solely on each input sequence separately, the query, key, and value-based attention mechanism introduces distinct roles for each token and incorporates separate linear transformations to enhance the attention process.\n",
    "\n",
    "In a transformer model, the query, key, and value are components of the self-attention mechanism used to compute attention scores between input elements. Overall, the query, key, and value enable the transformer to dynamically focus its attention on the most relevant parts of the input sequence, allowing for more effective processing and prediction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query: \n",
    "- The query vector represents the current token and is used to identify the parts of the input sequence that are most relevant to the current task. It captures the information necessary for the model to make predictions or generate output. \n",
    "- An input token vector is compared to every other vector to establish the weights for its own output. Query is used to identify the parts of the input sequence that are most relevant to the current task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key: \n",
    "- The key vector represents other tokens in the input sequence and is used to \"answer\" the query by computing a similarity score between the query and each key vector. The key vectors encode the information about the relationships between tokens in the sequence. \n",
    "- An input token vector is compared to every other vector to establish the weights for the output of those vectors. Keys are used to \"answer\" the query by computing a similarity score between the query and each key vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value: \n",
    "- The value vector provides additional information associated with each token and is used as part of the weighted sum to compute the output of the attention mechanism. The value vectors capture the content or context of each token. \n",
    "- An input token vector vectors is used as part of the weighted sum to compute each output vector once the weights have been established, i.e., to compute the output of the attention mechanism. Specifically, the attention scores between the query and key are used to weight the value vectors, and then a weighted sum of the values is computed to produce the output.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention mechanism\n",
    "\n",
    "The linear transformation for the query, key and value self-attention mechanism for each token $x_i$ can be defined as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q_i = W_qx_i \\enskip , \\quad k_i & = W_kx_i \\enskip , \\quad v_i = W_vx_i \\\\\n",
    "w'_{ij} & = q_i^Tk_j \\\\\n",
    "w_{ij} & = \\text{softmax}(w'_{ij}) \\\\\n",
    "y_i & = \\sum_jw_{ij}v_j \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $q$ is the query matrix, $k$ the key matrix and $v$ the value matrix. The $W_q$, $W_k$ and $W_v$ are $d_k \\times d_k$ weight matrices, where $d_k$ is the number of embedding dimensions.. The $w'_{ij}$ are the raw weights, $w_{ij}$ the scaled weights and $y_i$ the output of the self-attention mechanism. \n",
    "\n",
    "Let's then work through an example of the self-attention qith queries, keys and values. We stick to just figuring out the process and leave any architectural definitions of layers and such for later. Let's again define $x$ as an input with two tokens, each being a four-dimensional embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x.shape': (2, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 2],\n",
       "       [3, 4, 3, 4]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2,1,2],[3,4,3,4]])\n",
    "print({\"x.shape\": x.shape})\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then define the weight matrix for the queries and calculate the query matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q.shape': (4, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.23068269, 6.23853766],\n",
       "       [0.5516514 , 1.39332151],\n",
       "       [3.39797841, 7.28216267],\n",
       "       [1.14603798, 2.95227015]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = x.shape[-1]\n",
    "W_q = np.random.random((d_k,d_k))\n",
    "q = np.dot(W_q, x.T)\n",
    "print({\"q.shape\": q.shape})\n",
    "q\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same with the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k.shape': (4, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.41049675,  7.65870809],\n",
       "       [ 3.21822546,  6.78692766],\n",
       "       [ 1.74981191,  4.13631379],\n",
       "       [ 4.70445857, 10.59898094]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_k = np.random.random((d_k,d_k))\n",
    "k = np.dot(W_k, x.T)\n",
    "print({\"k.shape\": k.shape})\n",
    "k\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v.shape': (4, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.4142163 , 7.5882181 ],\n",
       "       [3.02124941, 6.80024817],\n",
       "       [3.14950406, 7.31213798],\n",
       "       [2.86978219, 6.73908348]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_v = np.random.random((d_k,d_k))\n",
    "v = np.dot(W_v, x.T)\n",
    "print({\"v.shape\": v.shape})\n",
    "v\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then calculate our weight matrix, applying softmax at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w.shape': (4, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.90270296e-17, 1.10440673e-19, 2.74849923e-28, 6.47419758e-08],\n",
       "       [9.74115109e-36, 2.60028783e-36, 2.87940198e-38, 1.19621843e-33],\n",
       "       [6.18805932e-12, 5.63274116e-15, 1.58821104e-25, 9.99999935e-01],\n",
       "       [1.13309143e-29, 6.93123722e-31, 5.14586158e-35, 2.93885032e-25]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "w = np.dot(q, k.T)\n",
    "w = softmax(w)\n",
    "print({\"w.shape\": w.shape})\n",
    "w\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, let's then produce our attention output. We have to transpose the output to make it conform to the input shape. This is important to enable stacking multiple attention layers on top of each other, as every attention layer expects inputs in similar shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y.shape': (2, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.85795369e-07, 3.47409154e-33, 2.86978200e+00, 8.43426811e-25],\n",
       "       [4.36301580e-07, 8.15322697e-33, 6.73908304e+00, 1.98060646e-24]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(w, v)\n",
    "y = y.T\n",
    "print({\"y.shape\": y.shape})\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we used only randomized weights. In a transformer model, these weights would be learned to best fit the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For addition reading, see the following resources:\n",
    " - [The Annotated Transformer: Encoder and Decoder stacks](http://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks)\n",
    " - [Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a#570c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the dot product\n",
    "\n",
    "The softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning, or cause it to stop altogether. Since the average value of the dot product grows with the embedding dimension $d_k$, it helps to scale the dot product back a little to stop the inputs to the softmax function from growing too large. Using the previous definition of query, key and value based self-attention, the scaling of the dot products is applied before the softmax function:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "w'_{ij} & = q_i^Tk_j \\\\\n",
    "\\text{here} \\to \\quad w_{ij}^{'} & = {q_i^Tk_j \\over \\sqrt{d_k}} \\\\\n",
    "w_{ij} & = \\text{softmax}(w'_{ij}) \\\\\n",
    "y_i & = \\sum_jw_{ij}v_j \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Citing from the original paper:\n",
    "> We suspect that for large values of $d_k$ [the number of embedding dimensions] the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by ${1 \\over \\sqrt{d_k}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For additional learning, see the following resources:\n",
    " - [Attention is all you need: 3.2.1 Scaled Dot-Product Attention](https://arxiv.org/pdf/1706.03762.pdf)\n",
    " - [The Annotated Transformer: Encoder and Decoder stacks](http://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks)\n",
    " - [Transformers from scratch: Additional tricks](https://peterbloem.nl/blog/transformers#additional-tricks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "In a single query, key and value self-attention operation, all information about the sequence just gets summed together. This means there is just a single way the influence of tokens to other tokens gets modelled. \n",
    "\n",
    "We can give the self attention greater power of discrimination, by combining several self-attention mechanisms (which we'll index with $r$), each with different matrices $W_q^r$, $W_k^r$ and $W_v^r$. These are called __attention heads__. For input token $x_i$ each attention head produces a different output vector $y_i^r$. These are then concatenated and passed through a linear transformation to reduce the dimension back to $k$.\n",
    "\n",
    "Practically this means, that the self-attention operation is just copied over as many times as there are heads. Each attention head gets its own separately initialized weight matrices. This helps the transformer model to learn multiple and different ways for the tokens to influence each other.\n",
    "\n",
    "Below is an illustration of how multi-head attention enables the retrieval of different representations of the same input sequence.\n",
    "\n",
    "![image](../../diagrams/multihead-attention.png)\n",
    "\n",
    "This and the transformer model will be imeplement in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-seminar-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
