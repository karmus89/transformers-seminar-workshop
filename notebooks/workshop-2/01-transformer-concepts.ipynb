{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer core concepts\n",
    "\n",
    "The outline of this notebook follow closely the blog post _TRANSFORMERS FROM SCRATCH_ by [Peter Boem](https://peterbloem.nl/blog/transformers) with some modifications and additions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher: Matrix multiplication\n",
    "\n",
    "Before we continue, however, it is a good idea to take a little refresher on _matrix multiplication_. Let's take matrices $M$ and $N$ as an example. $M$ has 2 rows ($i$) and 4 columns ($j$), while $N$ has 4 rows and 2 columns. \n",
    "\n",
    "To multiply these matrices, we start by taking the dot product ($\\times$) of the first row of $M$ with the first column of $N$. To do this, we multiply the first element in the first row of $M$ with the first element in the first column of $N$, then add the result to the product of the second element in the first row of $M$ and the second element in the first column of $N$, and so on for all four elements in the row and column.\n",
    "\n",
    "We repeat this process for all rows in $M$ and all columns in $N$, and write the results into a new matrix that has the same number of rows as $M$ and the same number of columns as $N$. In this example, the resulting matrix would have 2 rows and 2 columns.\n",
    "\n",
    "The above process is depicted below.\n",
    "\n",
    "![image](../../diagrams/matrix-multiplication.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding vectors\n",
    "\n",
    "The intuition behind embedding tokens to a vector is to represent each word or subword in a text as a numerical vector that captures its meaning and relationship to other words in the text. This is the basic idea behind word embeddings and subword embeddings, which are widely used in natural language processing (NLP) tasks.\n",
    "\n",
    "The key intuition behind embeddings is that words or subwords that have similar meanings or are used in similar contexts should be represented by similar or nearby embedding vectors in the high-dimensional space. This allows the embedding vectors to capture semantic and syntactic relationships between words, such as synonyms, antonyms, and analogies. For example, the embedding vectors for \"dog\" and \"cat\" might be closer together than the embedding vectors for \"dog\" and \"car\", reflecting their semantic similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define some example vocabulary and corresponding embedding vectors\n",
    "vocab = [\"cat\", \"dog\", \"car\", \"tree\"]\n",
    "embeddings = np.array(\n",
    "    [[0.3, 0.2, -0.1], [0.1, 0.4, 0.2], [-0.2, 0.1, 0.4], [0.1, -0.1, 0.3]]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to plot the embeddings in 3D space\n",
    "def plot_embeddings_3d(vocab, embeddings):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(\n",
    "        111,\n",
    "        projection=\"3d\",\n",
    "    )\n",
    "    ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2])\n",
    "    for i, word in enumerate(vocab):\n",
    "        ax.text(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], word)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the embeddings in 3D space\n",
    "plot_embeddings_3d(vocab, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings can be learned from scratch on a specific task or dataset, or they can be pre-trained on large amounts of unlabeled data and then fine-tuned on smaller labeled datasets for specific tasks. Pre-trained embeddings, such as those used in BERT, have been shown to be highly effective for a wide range of NLP tasks and are widely used in the field. What is most important is that with embeddings text can be given to neural network in numerical format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "The intuition behind positional encoding is to inject some notion of word order or position into the embedding vectors, which is necessary for sequence-to-sequence models like transformers that process variable-length input sequences.\n",
    "\n",
    "In natural language processing (NLP), word order is critical to understanding the meaning of a sentence or paragraph. However, traditional word embeddings like word2vec or GloVe do not capture any information about word order or position in the input sequence. This means that a traditional embedding alone would not be sufficient for a model like a transformer to fully understand a given text.\n",
    "\n",
    "Positional encoding addresses this limitation by adding an additional vector to the word embeddings that encodes the position of each word in the sequence. This vector is added to the input embeddings before they are passed through the transformer layers, allowing the model to better understand the relationships between words based on their position in the sequence.\n",
    "\n",
    "The formula for computing the positional encoding vector for a word at a given position $\\text{pos}$ and dimension $i$ is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "PE_{pos,2i} & = \\text{sin} \\left( \\frac{pos}{10000^{2i/d_{embed}}} \\right)\\text{,  if }i\\text{ is even} \\\\\n",
    "PE_{pos,2i+11} & = \\text{cos} \\left( \\frac{pos}{10000^{2i/d_{embed}}} \\right)\\text{,  if }i\\text{ is odd} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Here, $\\text{pos}$ is the position of the word in the sequence, $i$ is the dimension of the positional encoding vector, and $d_{embed}$ is the dimension of the input embeddings. The value 10000 is a hyperparameter that determines the scale of the sine and cosine functions. \n",
    "\n",
    "Let's use an example. Let's assume that the word ``hello`` maps to an embedding vector ``[0.1, 0.2]``. Below is in an illustration how applying positional encoding to the vector in different changes the original embedding values. In other words, the the sentence we're using is\n",
    "\n",
    "        [\"hello\", \"hello\", \"hello\", \"hello\"]\n",
    "\n",
    "or\n",
    "\n",
    "        [[0.1, 0.2], [0.1, 0.2], [0.1, 0.2], [0.1, 0.2]]\n",
    "\n",
    "Let's see how the vectors change when the position of the token changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def positional_encoding(pos, embedding_dim):\n",
    "    pe = np.zeros((embedding_dim,))\n",
    "    for i in range(0, embedding_dim, 2):\n",
    "        pe[i] = np.sin(pos / (10000 ** (i / embedding_dim)))\n",
    "        pe[i + 1] = np.cos(pos / (10000 ** ((i + 1) / embedding_dim)))\n",
    "    return pe\n",
    "\n",
    "\n",
    "embedding = [0.1, 0.2]\n",
    "embeddings = np.array([embedding, embedding, embedding, embedding])\n",
    "\n",
    "pos_encodings = np.zeros((len(embeddings), len(embedding)))\n",
    "for pos in range(len(embeddings)):\n",
    "    pos_encodings[pos] = positional_encoding(pos, len(embedding))\n",
    "\n",
    "pos_embed = embeddings + pos_encodings\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "for i in range(len(embeddings)):\n",
    "    ax.scatter(pos_embed[i, 0], pos_embed[i, 1], label=f\"Position {i}\")\n",
    "ax.set_title(\"Positional Embedding:\\n Same token, different positions  \")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General intuition\n",
    "\n",
    "The attention mechanism is like a teacher who helps you focus on what's important in a classroom. In the same way, the attention mechanism helps the transformer focus on which parts of the input it should be paying attention to, when processing information.\n",
    "\n",
    "The attention mechanism in transformers allows the model to weigh the importance of different parts of the input, giving more emphasis to the parts that are more relevant to the output.\n",
    "\n",
    "This is done by computing **attention scores** between each pair of input elements (e.g. words in a sentence) and then using these scores to compute a **weighted sum of the input elements**. The attention **scores are computed using a learned function (_neural network_)** that takes as input both the current input element and a \"query\" vector that represents the current state of the model. The resulting **weighted sum is then used as input** to the next layer of the transformer. \n",
    "\n",
    "Below is an illustration of how an input sequence $x$ is weighted when fed to a transformer's $TF$ first layer $TF_0$.\n",
    "\n",
    "![image](../../diagrams/attention.png)\n",
    "\n",
    "The attention mechanism in a transformer-based architecture allows the model to analyze all tokens in an input sequence simultaneously, unlike recurrent neural networks (RNNs) that process tokens sequentially. In RNNs, the model passes information from one token to the next through a hidden state, making it difficult to parallelize computation across the sequence. In contrast, transformers apply an attention mechanism that allows the model to look at all tokens in the sequence at once, and to dynamically weigh the importance of each token based on its relationship to the other tokens in the sequence. This parallelization across the sequence enables transformer models to process longer sequences more efficiently than RNNs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic self-attention\n",
    "\n",
    "We’ll start by implementing this basic self-attention operation in Pytorch. The first thing we should do is work out how to express the self attention in matrix multiplications. \n",
    "\n",
    "There are no parameters in basic self-attention (yet). What the basic self-attention actually does is entirely determined by whatever mechanism creates the input sequence. Upstream mechanisms, like an embedding layer, drive the self-attention by learning representations with particular dot products (although we’ll add a few parameters later).\n",
    "\n",
    "Self attention sees its input as a set, not a sequence. If we permute the input sequence, the output sequence will be exactly the same, except permuted also (i.e. self-attention is permutation equivariant). We will mitigate this somewhat when we build the full transformer, but the self-attention by itself actually ignores the sequential nature of the input.\n",
    "\n",
    "For an input sequence $x$, basic self-attention for each token $x_i$ can be defined as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "w'_i & = x_i^Tx_i \\\\\n",
    "w_i & = \\text{softmax}(w'_i) \\\\\n",
    "y_i & =  w_ix_i \\\\\n",
    "\\end{aligned}$$\n",
    "where $w'_i$ are the raw weights, $w_i$ the scaled weights and $y_i$ the output of the self-attention mechanism. \n",
    "> __From this definition it is explicit, that the self-attention is calculated for each input token separately, i.e. the context of the whole sequence is not used.__\n",
    "\n",
    "Using the above definitions, the illustrated process of perorming basic self-attention on an input is depicted below. First the raw weights $w'_i$ are calculated:\n",
    "\n",
    "![image](../../diagrams/basic-self-attention-raw-weight.png)\n",
    "\n",
    "Next, the raw weights are scaled:\n",
    "\n",
    "![image](../../diagrams/basic-self-attention-weight.png)\n",
    "\n",
    "Lastly, the input is weighted, producing the self-attention output:\n",
    "\n",
    "![image](../../diagrams/basic-self-attention-output.png)\n",
    "\n",
    "Let's define a function to demonstrate basic self-attention. The `apply_self_attention` function generates random sequences of embeddings, computes raw and scaled weights for each token in the sequence using the self-attention mechanism, and applies the attention to the embeddings to obtain an attention-weighted output.\n",
    "\n",
    "Additionally, we are using the `visualize_self_attention` function from the `visualize.py` in the current workshop notebook folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from visualize import visualize_self_attention\n",
    "\n",
    "\n",
    "def apply_self_attention(\n",
    "    sequences, tokens_in_sequence, embedding_vector_dims, visualize=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply self-attention mechanism to a random sequence of embeddings.\n",
    "    \"\"\"\n",
    "    x = torch.rand(sequences, tokens_in_sequence, embedding_vector_dims)\n",
    "    print({\"x.shape\": x.shape})\n",
    "\n",
    "    raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "    print({\"raw_weights.shape\": raw_weights.shape})\n",
    "\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "    print({\"weights.shape\": weights.shape})\n",
    "\n",
    "    y = torch.bmm(weights, x)\n",
    "    print({\"y.shape\": y.shape})\n",
    "\n",
    "    if visualize:\n",
    "        visualize_self_attention(x, raw_weights, weights, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then try to visualize what the attention does. Let's begin by looking at a single token sequence. The legend for notation is the following:\n",
    " - $S_0$ is the first sequence of tokens in the input batch\n",
    " - $T$ are the token-wise embedding vectors in an input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_self_attention(sequences=1, tokens_in_sequence=1, embedding_vector_dims=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the softmax scaling function having only a single value to operate upon, the self attention output conforms to the input completely. Weight calculation is just a simple sequence-wise matrix multiplication.\n",
    "\n",
    "Next, let's take a look at two-token sequence. Note though, that the visualizations consider only the first sequence in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_self_attention(sequences=1, tokens_in_sequence=2, embedding_vector_dims=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we already see difference between self-attention outputs and inputs. The effects of matrix multiplication show better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the dot product\n",
    "\n",
    "The softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning, or cause it to stop altogether. Since the average value of the dot product grows with the embedding dimension k, it helps to scale the dot product back a little to stop the inputs to the softmax function from growing too large:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "w_{ij}^{'} & = {q_i^Tk_j \\over \\sqrt{k}} \n",
    "\\end{aligned}$$\n",
    "\n",
    "> Why $\\sqrt{k}$? Imagine a vector in ℝk with values all c. Its Euclidean length is $\\sqrt{kc}$. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors.\n",
    "\n",
    "As the change is minor, its enough to just show the change in code:\n",
    "\n",
    "```python\n",
    "    \n",
    "    # ...\n",
    "    # stuff done before in the function\n",
    "\n",
    "    raw_weights = torch.bmm(x, x.transpose(1, 2))/torch.sqrt(embedding_vector_dims)\n",
    "    print({\"raw_weights.shape\": raw_weights.shape})\n",
    "\n",
    "    # stuff done after in the function\n",
    "    # ...\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries, keys and values\n",
    "\n",
    "In the basic self-attention we've seen so far, each input vector must play all three roles. We make its life a little easier by deriving new vectors for each role, by applying a linear transformation to the original input vector. \n",
    "\n",
    "In a transformer model, the query, key, and value are components of the self-attention mechanism used to compute attention scores between input elements. Overall, the query, key, and value enable the transformer to dynamically focus its attention on the most relevant parts of the input sequence, allowing for more effective processing and prediction. \n",
    "\n",
    "- __Query__: An input token vector is compared to every other vector to establish the weights for its own output . Query is used to identify the parts of the input sequence that are most relevant to the current task.\n",
    "\n",
    "$$x_i \\to y_i$$\n",
    "\n",
    "- __Key__: An input token vector is compared to every other vector to establish the weights for the output of those vectors. Keys are used to \"answer\" the query by computing a similarity score between the query and each key vector.\n",
    "\n",
    "$$x_i \\to y_j$$\n",
    "\n",
    "- __Value__: An input token vector vectors is used as part of the weighted sum to compute each output vector once the weights have been established, i.e., to compute the output of the attention mechanism. Specifically, the attention scores between the query and key are used to weight the value vectors, and then a weighted sum of the values is computed to produce the output.\n",
    "\n",
    "The linear transformation for the query, key and value self-attention mechanism can be defined as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q_i = W_qx_i \\enskip , \\quad k_i & = W_kx_i \\enskip , \\quad v_i = W_vx_i \\\\\n",
    "w'_{ij} & = q_i^Tk_j \\\\\n",
    "w_{ij} & = \\text{softmax}(w'_{ij}) \\\\\n",
    "y_i & = \\sum_jw_{ij}v_j \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $q$ is the query matrix, $k$ the key matrix and $v$ the value matrix. The $W_q$, $W_k$ and $W_v$ are $k \\times k$ weight matrices. The $w'_{ij}$ are the raw weights, $w_{ij}$ the scaled weights and $y_i$ the output of the self-attention mechanism. \n",
    "\n",
    "> __From this definition we can see, that by the use of key vectors the context of the whole sequence is taken into account when calculating token-wise self-attention outputs.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then define an example of the self-attention qith queries, keys and values. We stick to just figuring out the process and leave any architectural definitions of layers and such for later. \n",
    "\n",
    "The `apply_self_attention_with_qkv` function applies the self-attention mechanism with query, key, and value components to a random sequence of embeddings. It first generates a random sequence of embeddings and three weight matrices for the query, key, and value components. Then, it computes the query, key, and value vectors by performing matrix multiplication between the weight matrices and the embeddings. The function then computes the raw and scaled weights for each token in the sequence using the self-attention mechanism, and applies the attention to the value vectors to obtain an attention-weighted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_self_attention_with_qkv(\n",
    "    sequences, tokens_in_sequence, embedding_vector_dims, visualize=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply self-attention mechanism with query, key, and value components to a random sequence of embeddings.\n",
    "    \"\"\"\n",
    "    x = torch.rand(sequences, tokens_in_sequence, embedding_vector_dims)\n",
    "    print({\"x.shape\": x.shape})\n",
    "\n",
    "    W_q = torch.rand(1, embedding_vector_dims, embedding_vector_dims)\n",
    "    W_k = torch.rand(1, embedding_vector_dims, embedding_vector_dims)\n",
    "    W_v = torch.rand(1, embedding_vector_dims, embedding_vector_dims)\n",
    "\n",
    "    q = torch.bmm(W_q, x.transpose(1, 2))\n",
    "    print({\"q.shape\": q.shape})\n",
    "    k = torch.bmm(W_k, x.transpose(1, 2))\n",
    "    print({\"k.shape\": k.shape})\n",
    "    v = torch.bmm(W_v, x.transpose(1, 2))\n",
    "    print({\"v.shape\": v.shape})\n",
    "\n",
    "    raw_weights = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(\n",
    "        torch.tensor(embedding_vector_dims)\n",
    "    )\n",
    "    print({\"raw_weights.shape\": raw_weights.shape})\n",
    "\n",
    "    weights = F.softmax(raw_weights, dim=2)\n",
    "    print({\"weights.shape\": weights.shape})\n",
    "\n",
    "    y = torch.bmm(weights, v).transpose(1, 2)\n",
    "    print({\"y.shape\": y.shape})\n",
    "\n",
    "    if visualize:\n",
    "        visualize_self_attention(\n",
    "            x,\n",
    "            raw_weights,\n",
    "            weights,\n",
    "            y,\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then see what happens with a two-token sequence with queries, keys and values implementation of the self-attention mechanism.  The legend for notation is the following:\n",
    " - $S_0$ is the first sequence of tokens in the input batch\n",
    " - $T$ are the token-wise embedding vectors in an input sequence\n",
    " - $Q$ are the query vectors\n",
    " - $K$ are the key vectors\n",
    " - $V$ are the value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_self_attention_with_qkv(\n",
    "    sequences=1, tokens_in_sequence=2, embedding_vector_dims=4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of changing single embedding values for each token, the tokens are now weighted in relation to other tokens. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "In a single query, key and value self-attention operation, all information about the sequence just gets summed together. This means there is just a single way the influence of tokens to other tokens gets modelled. \n",
    "\n",
    "We can give the self attention greater power of discrimination, by combining several self-attention mechanisms (which we'll index with $r$), each with different matrices $W_q^r$, $W_k^r$ and $W_v^r$. These are called __attention heads__. For input token $x_i$ each attention head produces a different output vector $y_i^r$. These are then concatenated and passed through a linear transformation to reduce the dimension back to $k$.\n",
    "\n",
    "The simplest way to understand multi-head self-attention is to see it as a small number of copies of the self-attention mechanism applied in parallel, each with their own key, value and query transformation. To accomplish this, each head receives low-dimensional keys queries and values. If the input vector has $k=256$ embedding dimensions, and we have $h=4$ attention heads, we multiply the input vectors by a $256\\times64$ matrix to project them down to a sequence of 64 dimensional vectors. For every head, we do this 3 times: for the keys, the queries and the values.\n",
    "\n",
    "Practically this means, that the self-attention operation is just copied over as many times as there are heads. Each attention head gets its own separately initialized weight matrices. This helps the transformer model to learn multiple and different ways for the tokens to influence each other.\n",
    "\n",
    "Below is an illustration of how multi-head attention enables the retrieval of different representations of the same input sequence.\n",
    "\n",
    "![image](../../diagrams/multihead-attention.png)\n",
    "\n",
    "This and the transformer model will be imeplement in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-seminar-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
