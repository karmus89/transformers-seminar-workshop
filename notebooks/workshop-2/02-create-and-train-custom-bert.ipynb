{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a BERT-like transformer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General architecture\n",
    "\n",
    "The BERT model consists of multiple transformer layers, where each layer is composed of multi-head self-attention and point-wise feedforward networks. The input to the model is a sequence of word-piece tokens, which are first passed through an embedding layer to generate dense vector representations of each token. These embeddings are then fed to the input of the first transformer layer. The output of the last transformer layer is then used as input to a task-specific layer, which can be a simple linear layer for classification tasks or a sequence labeling layer for sequence tagging tasks, among others.\n",
    "\n",
    "The following are the contents of a single BERT transformer layer:\n",
    "\n",
    "1. Multi-Head Self-Attention: This sublayer consists of multiple parallel attention mechanisms (heads) that attend to different parts of the input sequence. The output of each head is concatenated and then passed through a linear projection to produce a single attention vector for each input token.\n",
    "\n",
    "2. Layer Normalization: This sublayer normalizes the outputs of the self-attention sublayer across the hidden dimension. Specifically, it applies a normalization operation that scales and shifts the outputs to have zero mean and unit variance.\n",
    "\n",
    "3. Point-wise Feedforward Network: This sublayer applies a fully connected feedforward network to each position in the sequence independently. The output of this sublayer is a new set of hidden representations for each position.\n",
    "\n",
    "4. Another Layer Normalization: As in step 2, this sublayer normalizes the outputs of the feedforward sublayer across the hidden dimension.\n",
    "\n",
    "5. Skip Connection and Residual Connection: This sublayer adds the output of the feedforward sublayer to the original input embeddings and applies a residual connection. This allows the model to directly incorporate the original input embeddings into the final output of the layer.\n",
    "\n",
    "Note that the output of each transformer layer is a sequence of hidden representations that are passed to the next layer in the stack. In BERT, multiple transformer layers are stacked on top of each other to form a deep neural network. The final output of the BERT model is typically the output of the last transformer layer, which can be used for downstream tasks such as text classification, named entity recognition, and question answering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between the BERT and the original transformer\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a variation of the original \"Attention is All You Need\" transformer model. The core idea behind both models is the same, i.e., to use self-attention to model the relationships between different positions in a sequence. However, BERT introduces several key modifications to the original transformer architecture that make it more effective for tasks such as natural language understanding.\n",
    "\n",
    "The original transformer was designed as an encoder-decoder model for machine translation, where the encoder takes in the source language sentence and produces a sequence of hidden states, and the decoder takes in these hidden states and generates the target language sentence. However, BERT is not an encoder-decoder model like the original transformer.\n",
    "\n",
    "Instead, BERT is an encoder-only model that is pre-trained on large amounts of unlabeled text data. The pre-training involves masking a certain percentage of the input tokens in each training example and training the model to predict the masked tokens based on the context provided by the other tokens in the input. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "The positional encoding (PE) scheme used in BERT is the same as the original transformer model, but there is a subtle difference in how the PE vectors are calculated. In the original transformer model, the PE vectors are calculated based on a fixed function that is defined separately from the model itself. Specifically, for each position in the input sequence, the PE vector is calculated based on a fixed set of sinusoidal functions with different frequencies and phases. The frequencies and phases are pre-defined and do not change during training.\n",
    "\n",
    "In contrast, in BERT, the PE vectors are learned during the model training process. Specifically, the PE vectors are initialized randomly and then fine-tuned along with the rest of the model parameters during the pre-training process. This allows the model to learn a more task-specific representation of the position information that is optimal for the downstream task.\n",
    "\n",
    "Another difference between BERT and the original transformer in terms of positional encoding is the way in which the position information is combined with the token embeddings. In the original transformer, the PE vectors are added directly to the token embeddings, while in BERT, the PE vectors are added after the token embeddings are passed through a layer normalization step. This means that the PE vectors in BERT have access to the normalized embeddings, which can help to mitigate the effect of covariate shift during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Loading the data is the first step in any data-driven machine learning project. In this case, we will use the Python pandas library to read in the training and validation data from their respective CSV files using the ``pd.read_csv()`` function. The data is expected to be stored in the specified file locations relative to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"../../data/train.csv\", index_col=0)\n",
    "df_validation = pd.read_csv(\"../../data/validation.csv\", index_col=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the embeddings\n",
    "\n",
    "First thing to do is to create our own embeddings and the embedding layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the vocabulary\n",
    "\n",
    "In order to use an embedding layer, we'll first need to create our own vocabulary. This involves tokenizing our text data into a sequence of tokens and assigning a unique index to each token in the vocabulary. Creating the vocabulary is an important step in any natural language processing (NLP) project. In this case, we will create a vocabulary from the training data that will be used to convert the text into a numerical format that can be fed into the BERT model.\n",
    "\n",
    "Next we extract the unique tokens from the training data and create a vocabulary for use in the BERT model. The resulting vocabulary will be used to map each word in the text data to a unique numerical ID, which will be used as input to the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 18143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nasopharyngoscopy',\n",
       " 'rosenmuller',\n",
       " 'diagnostic',\n",
       " 'pursuit',\n",
       " 'strengthened',\n",
       " '20l',\n",
       " 'hyperhidrosis',\n",
       " '72',\n",
       " 'reoccurrence',\n",
       " 'bike',\n",
       " 'derived',\n",
       " 'design',\n",
       " 'sulfate',\n",
       " 'orifice',\n",
       " 'adiposity',\n",
       " 'indicator',\n",
       " 'lab',\n",
       " 'gained',\n",
       " 'qam',\n",
       " 'reducer',\n",
       " 'adenomatous',\n",
       " 'hemodilutional',\n",
       " 'tibiales',\n",
       " 'rf',\n",
       " 'decrescendo']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = set()\n",
    "for i, row in df_train.iterrows():\n",
    "    tokens = row[\"transcription\"].split(\" \")\n",
    "    for token in tokens:\n",
    "        unique_tokens.add(token)\n",
    "\n",
    "print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "unique_tokens = list(unique_tokens)\n",
    "\n",
    "unique_tokens[:25]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a mapping from each unique token to a unique integer ID, which will be used to represent the token in the numerical format. \n",
    "\n",
    "We start by defining several special tokens that are commonly used in NLP tasks, including the classification token ``[CLS]``, the separator token ``[SEP]``, the unknown token ``[UNK]``, and the padding token ``[PAD]``. These tokens will be added to the beginning of the vocabulary along with the unique tokens extracted from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = \"[CLS]\"\n",
    "sep_token = \"[SEP]\"\n",
    "unknown_token = \"[UNK]\"\n",
    "padding_token = \"[PAD]\"\n",
    "vocabulary = {}\n",
    "idx = 0\n",
    "tokens = [cls_token, sep_token, unknown_token, padding_token] + unique_tokens\n",
    "for token in tokens:\n",
    "    vocabulary[token] = idx\n",
    "    idx += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping tokens to indices\n",
    "\n",
    "Next, we will map each token in the text data to its corresponding index in the vocabulary. To achieve this, we will define two functions.\n",
    "\n",
    " - ``map_token_to_vocabulary_index()``: takes a single token as input and returns the index of the token in the vocabulary dictionary. If the token is not present in the vocabulary, the function returns the index of the unknown token ``[UNK]``.\n",
    "\n",
    " - ``map_seq_to_vocabulary()``: takes a string of text as input, splits it into individual tokens, and applies the ``map_token_to_vocabulary_index()`` function to each token using the built-in ``map()`` function. This function returns a list of vocabulary indices corresponding to each token in the input text.\n",
    "\n",
    "These functions rely on the ``vocabulary`` dictionary that was created in the previous step, which maps each token to a unique numerical ID. The resulting list of vocabulary indices can be used as input to the BERT model, where each index corresponds to a particular token in the input text.\n",
    "\n",
    "This step is important as it allows us to convert the raw text data into a numerical format that can be fed into the BERT model. The resulting numerical sequences will be used as input to the BERT model during both the pre-training and fine-tuning stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_token_to_vocabulary_index(token: str):\n",
    "    try:\n",
    "        idx = vocabulary[token]\n",
    "    except KeyError:\n",
    "        idx = vocabulary[unknown_token]\n",
    "    return idx\n",
    "\n",
    "\n",
    "def map_seq_to_vocabulary(text: str):\n",
    "    tokens = text.split(\" \")\n",
    "    return list(map(map_token_to_vocabulary_index, tokens))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions, we will create new columns in the pandas DataFrame objects to store the resulting numerical indices. After running this code, the ``df_train`` and ``df_validation`` DataFrames will contain a new column called ``idx_labels``, which stores the numerical indices for each row of text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"idx_labels\"] = df_train[\"transcription\"].apply(map_seq_to_vocabulary)\n",
    "df_validation[\"idx_labels\"] = df_validation[\"transcription\"].apply(\n",
    "    map_seq_to_vocabulary\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and trimming to max length\n",
    "\n",
    "The next step is to prepare the numerical sequences of token indices for input to the BERT model. To do this, we will define a function that will trim or pad each sequence to a fixed maximum length.\n",
    "\n",
    "This function is important as it allows us to ensure that each input sequence to the BERT model has the same length, which is required for efficient batch processing during training. By trimming or padding each sequence to a fixed length, we can ensure that all sequences can be processed efficiently in parallel by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remove this cell\n",
    "MAX_LEN = 128\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function, called ``trim_or_pad()``, takes a list of numerical indices as input and performs the following operations:\n",
    "\n",
    "1. Inserts the index of the classification token ``[CLS]`` at the beginning of the sequence.\n",
    "1. If the sequence length is greater than the maximum length ``MAX_LEN``, it trims the sequence to the maximum length by discarding tokens from the end of the sequence.\n",
    "1. Replaces the last token in the sequence with the index of the separator token ``[SEP].``\n",
    "1. If the sequence length is less than the maximum length, it pads the sequence with the index of the padding token ``[PAD]`` to the maximum length.\n",
    "1. The ``MAX_LEN`` parameter is a fixed maximum sequence length that we have chosen for the BERT model. Any sequences longer than ``MAX_LEN`` will be trimmed, and any sequences shorter than ``MAX_LEN`` will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_or_pad(indices: list):\n",
    "    cls_idx = vocabulary[cls_token]\n",
    "    indices.insert(0, cls_idx)\n",
    "\n",
    "    if len(indices) > MAX_LEN:\n",
    "        indices = indices[:MAX_LEN]\n",
    "\n",
    "    sep_idx = vocabulary[sep_token]\n",
    "    indices[-1] = sep_idx\n",
    "\n",
    "    padding_idx = vocabulary[padding_token]\n",
    "    if len(indices) < MAX_LEN:\n",
    "        padding = MAX_LEN - len(indices)\n",
    "        indices = indices + [padding_idx] * padding\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204     [0, 12033, 6580, 8336, 15449, 6867, 3210, 1138...\n",
       "1561    [0, 15080, 7934, 16750, 5923, 13737, 13427, 10...\n",
       "1564    [0, 15080, 7934, 9839, 14576, 12373, 5931, 138...\n",
       "1072    [0, 12033, 6580, 16750, 4925, 8646, 7747, 2161...\n",
       "4193    [0, 7007, 5422, 1780, 6709, 9081, 9178, 5931, ...\n",
       "Name: idx_labels, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"idx_labels\"] = df_train[\"idx_labels\"].apply(trim_or_pad)\n",
    "df_validation[\"idx_labels\"] = df_validation[\"idx_labels\"].apply(trim_or_pad)\n",
    "df_train[\"idx_labels\"].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After mapping the tokens to their corresponding indices in the vocabulary and applying padding and trimming to the sequences, we have now prepared our data to be fed into the BERT model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding mask\n",
    "\n",
    "The last part of modifying our data for embedding is to generate a mask for informing the model about padded values. This is similar to how the BERT we used in the first workshop has been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = vocabulary[padding_token]\n",
    "df_train[\"attention_mask\"] = df_train[\"idx_labels\"].apply(\n",
    "    lambda tokens: [token != padding_idx for token in tokens]\n",
    ")\n",
    "df_validation[\"attention_mask\"] = df_validation[\"idx_labels\"].apply(\n",
    "    lambda tokens: [token != padding_idx for token in tokens]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "mask_arr = np.array(df_train.iloc[:, -1:].values.tolist()).squeeze(1)\n",
    "sns.heatmap(mask_arr, cbar=False, xticklabels=[], yticklabels=[])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TranscriptionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TranscriptionEmbedding, self).__init__()\n",
    "        padding_idx = vocabulary[padding_token]\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding output shape: torch.Size([128, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-8.2848e-01, -2.3622e-01, -8.6701e-01,  ...,  9.9628e-01,\n",
       "          3.9725e-01, -2.8840e-01],\n",
       "        [ 1.2859e-01, -4.9964e-01, -1.1247e+00,  ...,  5.8802e-01,\n",
       "         -8.2757e-01, -3.3755e-02],\n",
       "        [-1.6918e+00, -3.5186e-06, -1.7359e+00,  ..., -8.5207e-01,\n",
       "         -8.4908e-01,  5.2583e-01],\n",
       "        ...,\n",
       "        [ 3.9337e-01, -9.7914e-01, -1.3223e+00,  ...,  2.2692e+00,\n",
       "          6.1488e-01,  1.2311e+00],\n",
       "        [ 3.6993e-01, -5.7487e-01, -1.7773e+00,  ..., -9.2903e-01,\n",
       "          1.1926e+00, -2.0399e-01],\n",
       "        [ 1.3671e+00, -2.9186e-01,  5.0808e-01,  ...,  3.6067e-01,\n",
       "          7.9539e-01,  8.4303e-01]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = TranscriptionEmbedding(vocab_size=len(vocabulary), embedding_dim=64)\n",
    "sample = torch.tensor(df_train[\"idx_labels\"].values[0])\n",
    "embedded_sample = embedding(sample)\n",
    "print(f\"Embedding output shape: {embedded_sample.shape}\")\n",
    "embedded_sample\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the BERT-like transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space\n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_size, 2).float() * (-math.log(10000.0) / hidden_size)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encoding to the input embeddings\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation computes the positional encodings using a formula based on sine and cosine functions. The positional encodings are then added to the input embeddings in the ``forward()`` method of the module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear projections\n",
    "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask=None):\n",
    "        batch_size, seq_length, hidden_size = input_tensor.size()\n",
    "\n",
    "        # Project inputs to queries, keys, and values\n",
    "        query = self.query_linear(input_tensor)\n",
    "        key = self.key_linear(input_tensor)\n",
    "        value = self.value_linear(input_tensor)\n",
    "\n",
    "        # Split heads\n",
    "        query = query.view(\n",
    "            batch_size, seq_length, self.num_heads, hidden_size // self.num_heads\n",
    "        )\n",
    "        key = key.view(\n",
    "            batch_size, seq_length, self.num_heads, hidden_size // self.num_heads\n",
    "        )\n",
    "        value = value.view(\n",
    "            batch_size, seq_length, self.num_heads, hidden_size // self.num_heads\n",
    "        )\n",
    "\n",
    "        # Transpose to get dimensions [batch_size, num_heads, seq_length, hidden_size // num_heads]\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        scores = scores / (hidden_size // self.num_heads) ** 0.5\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), -1e9)\n",
    "\n",
    "        # Apply softmax to obtain attention weights\n",
    "        weights = nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # Concatenate heads and apply output projection\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "        output = self.output_linear(context)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module takes an input tensor of shape ``(batch_size, seq_length, hidden_size)`` and applies multi-head self-attention to the input. The ``hidden_size`` parameter determines the dimensionality of the input and output tensors, and the ``num_heads`` parameter determines the number of parallel attention mechanisms (heads) to use.\n",
    "\n",
    "The module first projects the input tensor to query, key, and value vectors using linear projection layers. It then splits the query, key, and value vectors into multiple heads using the ``view()`` method and transposes the resulting tensor so that the head dimension comes before the sequence and hidden dimensions.\n",
    "\n",
    "The module then computes attention scores by taking the dot product of the query and key tensors, scaling the result by the square root of the hidden size, and applying a mask to the scores to prevent attending to padding tokens. The module applies a softmax function to the attention scores along the head and sequence dimensions to obtain attention weights for each token in the sequence.\n",
    "\n",
    "The module then multiplies the attention weights by the value vectors, concatenates the resulting tensor along the head dimension, and passes the result through a linear projection layer to obtain the output tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the multi-head attention module is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: torch.Size([1, 128])\n",
      "Input mask: torch.Size([1, 128])\n",
      "Embedded input sample: torch.Size([1, 128, 64])\n",
      "Multi-head attention output: torch.Size([1, 128, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0128,  0.0018, -0.0713,  ...,  0.0845, -0.0837,  0.0134],\n",
       "         [-0.0128,  0.0018, -0.0713,  ...,  0.0845, -0.0837,  0.0134],\n",
       "         [-0.0128,  0.0018, -0.0713,  ...,  0.0845, -0.0837,  0.0134],\n",
       "         ...,\n",
       "         [-0.0128,  0.0018, -0.0713,  ...,  0.0845, -0.0837,  0.0134],\n",
       "         [-0.0128,  0.0018, -0.0713,  ...,  0.0845, -0.0837,  0.0134],\n",
       "         [-0.0128,  0.0018, -0.0713,  ...,  0.0845, -0.0837,  0.0134]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.tensor([df_train[\"idx_labels\"].values[0]])\n",
    "print(f\"Input sample: {sample.shape}\")\n",
    "\n",
    "mask = torch.tensor([df_train[\"attention_mask\"].values[0]])\n",
    "print(f\"Input mask: {mask.shape}\")\n",
    "\n",
    "embedding = TranscriptionEmbedding(vocab_size=len(vocabulary), embedding_dim=64)\n",
    "attention = MultiHeadAttention(hidden_size=64, num_heads=4)\n",
    "\n",
    "embedded_sample = embedding(sample)\n",
    "print(f\"Embedded input sample: {embedded_sample.shape}\")\n",
    "\n",
    "output = attention(embedded_sample, mask)\n",
    "print(f\"Multi-head attention output: {output.shape}\")\n",
    "output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ff_size):\n",
    "        super().__init__()\n",
    "        self.multihead_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.pointwise_feedforward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, ff_size),\n",
    "            nn.Linear(ff_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        attention_output = self.multihead_attention(x, attention_mask)\n",
    "        norm1_output = self.layer_norm1(x + attention_output)\n",
    "        ff_output = self.pointwise_feedforward(norm1_output)\n",
    "        norm2_output = self.layer_norm2(norm1_output + ff_output)\n",
    "        return norm2_output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``TransformerLayer`` module takes an input tensor ``x`` of shape ``(batch_size, seq_length, hidden_size)`` and an optional padding mask ``attention_mask`` of shape ``(batch_size, seq_length)``. \n",
    "\n",
    "It first applies the multi-head self-attention sublayer to the input tensor, passing the padding mask as an additional argument. It then applies the first layer normalization sublayer, adds the output of the multi-head self-attention sublayer to the input tensor using a skip connection, and applies the second layer normalization sublayer. Finally, it passes the output through the point-wise feedforward sublayer and adds the resulting tensor to the output of the second layer normalization sublayer using another skip connection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the layer in action, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer layer output: torch.Size([1, 128, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1997, -0.9157, -0.4273,  ..., -0.7983, -0.6273, -0.1647],\n",
       "         [-2.3000,  0.3583, -1.1448,  ...,  0.4278,  0.2942, -0.6848],\n",
       "         [ 1.4600,  0.0072, -1.7019,  ...,  0.2434,  1.7239, -0.4886],\n",
       "         ...,\n",
       "         [-1.3621, -0.7635, -0.2736,  ...,  1.3146, -1.0958, -0.4531],\n",
       "         [-1.0960, -1.3660,  0.3684,  ...,  0.2475,  0.1249,  1.3666],\n",
       "         [ 0.8116,  2.5123, -1.0757,  ..., -0.9678, -1.4988, -1.2642]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.tensor([df_train[\"idx_labels\"].values[0]])\n",
    "mask = torch.tensor([df_train[\"attention_mask\"].values[0]])\n",
    "\n",
    "embedding = TranscriptionEmbedding(vocab_size=len(vocabulary), embedding_dim=64)\n",
    "transformer_layer = TransformerLayer(hidden_size=64, num_heads=4, ff_size=128)\n",
    "\n",
    "embedded_sample = embedding(sample)\n",
    "output = transformer_layer(embedded_sample, mask)\n",
    "print(f\"Transformer layer output: {output.shape}\")\n",
    "output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-like transformer\n",
    "\n",
    "In this implementation, we create an instance of the BERT module with the following hyperparameters:\n",
    "\n",
    "- ``vocab_size``: The size of the vocabulary for the input sequence.\n",
    "- ``hidden_size``: The dimensionality of the input and output tensors.\n",
    "- ``num_layers``: The number of transformer layers to stack.\n",
    "- ``num_heads``: The number of parallel attention mechanisms to use in each transformer layer.\n",
    "- ``ff_size``: The size of the feedforward network used in each transformer layer.\n",
    "- ``max_len``: The maximum length of the input sequence.\n",
    "\n",
    "We first create an input embedding layer using ``TranscriptionEmbedding``. We then create a ``PositionalEncoding`` module that adds position information to the input embeddings.\n",
    "\n",
    "We then create a stack of transformer layers using ``nn.ModuleList`` and the ``TransformerLayer`` module. We also create an output layer for classification using ``nn.Linear``.\n",
    "\n",
    "In the ``forward()`` method, we first apply the input embedding and positional encoding to the input tensor. We then apply each of the transformer layers in turn. Finally, we apply the classification layer to the output tensor, using only the first token (the ``[CLS]`` token) for classification.\n",
    "\n",
    "Note that this implementation is simplified and does not include all of the features of a full BERT model. For example, BERT includes a separate output layer for each token in the input sequence, and uses a more complex classification scheme than the simple linear layer used here. However, this implementation should give you an idea of how to use the ``TransformerLayer`` module to create a BERT-like architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_classes,\n",
    "        hidden_size=768,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        ff_size=3072,\n",
    "        max_len=512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = TranscriptionEmbedding(vocab_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, max_len)\n",
    "        self.transformer_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(hidden_size, num_heads, ff_size)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x, attention_mask)\n",
    "        x = self.classifier(x[:, 0, :])  # use the first token for classification\n",
    "        if labels is not None:\n",
    "            loss = self.loss(x, labels)\n",
    "            return {\"loss\": loss, \"logits\": x}\n",
    "        return {\"logits\": x}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BERT-like transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204     0\n",
       "1561    1\n",
       "1564    1\n",
       "1072    0\n",
       "4193    2\n",
       "Name: medical_specialty_label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_label = {\n",
    "    \"Surgery\": 0,\n",
    "    \"Radiology\": 1,\n",
    "    \"Consult\": 2,\n",
    "    \"Cardiovascular\": 3,\n",
    "    \"Orthopedic\": 4,\n",
    "    \"General Medicine\": 5,\n",
    "}\n",
    "df_train[\"medical_specialty_label\"] = df_train[\"medical_specialty\"].map(class_to_label)\n",
    "df_validation[\"medical_specialty_label\"] = df_validation[\"medical_specialty\"].map(\n",
    "    class_to_label\n",
    ")\n",
    "\n",
    "df_train[\"medical_specialty_label\"].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MedicalTranscriptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.tokens = torch.tensor(df[\"idx_labels\"].values.tolist()).long()\n",
    "        self.attention_mask = torch.tensor(df[\"attention_mask\"].values.tolist())\n",
    "        self.labels = torch.tensor(df[\"medical_specialty_label\"].values.tolist())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokens[idx].clone().detach(),\n",
    "            \"attention_mask\": self.attention_mask[idx].clone().detach(),\n",
    "            \"labels\": self.labels[idx].clone().detach(),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample type: <class 'dict'>\n",
      "Sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 15080,  7934, 16750,  5923, 13737, 13427, 10261,  3231,  5931,\n",
       "         13957, 16535,  8749, 13012,  2310,  3077, 14975,  1906,  6335, 14576,\n",
       "          7545, 14585,  6387, 11809, 17659, 13725, 13766, 11421,  4033, 15168,\n",
       "          8180,  7177,  9654,  9686, 13925,  1180,  6306,  9686, 16068,  2806,\n",
       "          6306,  5049, 17842,  3136,  9548, 15371,  6306,  2366, 14773,  2515,\n",
       "          6335,  3488,  1236,  6306,  5025,   350, 14881,  6387, 17460,  2806,\n",
       "          1196, 16118,  4033, 15168,  8180,  7177,  5613,  4033,  6235,  8180,\n",
       "         10989,  7952, 15027,  5533,  3220,  9404,  6922, 17832,  4939, 16459,\n",
       "         17320, 14773, 12935,  4033,  6235,  8180,  2997, 11941,  3618, 15027,\n",
       "         13375,  9539, 11041, 11529, 11126,  3114, 11291,  1518,  1386, 16196,\n",
       "          8564,  3776, 11832,   447,  4033, 17765,  8180, 10989, 14141,  4033,\n",
       "          6235,  8180,  1018, 12369,  9548, 13375,  9056, 14786, 16854, 10594,\n",
       "          8536, 15492, 15356, 15492, 16940,  4081,  9665,     1]),\n",
       " 'attention_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MedicalTranscriptionDataset(df=df_train)\n",
    "sample = dataset[1]\n",
    "print(f\"Sample type: {type(sample)}\")\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(1.6430, grad_fn=<NllLossBackward0>),\n",
       " 'logits': tensor([[-0.1910,  0.1175,  0.1654, -0.0849, -0.2125, -0.0433]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT(\n",
    "    vocab_size=len(vocabulary),\n",
    "    num_classes=len(df_train[\"medical_specialty_label\"].unique()),\n",
    "    hidden_size=64,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    ff_size=128,\n",
    "    max_len=MAX_LEN,\n",
    ")\n",
    "model(\n",
    "    input_ids=sample[\"input_ids\"].unsqueeze(0),\n",
    "    attention_mask=sample[\"attention_mask\"].unsqueeze(0),\n",
    "    labels=sample[\"labels\"].unsqueeze(0),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grimfada/miniconda3/envs/transformers-seminar-workshop/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2290\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 288\n",
      "  Number of trainable parameters = 10972422\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 00:52, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(\n",
    "    model=BERT(\n",
    "        vocab_size=len(vocabulary),\n",
    "        num_classes=len(df_train[\"medical_specialty_label\"].unique()),\n",
    "        hidden_size=256,\n",
    "        num_layers=12,\n",
    "        num_heads=8,\n",
    "        ff_size=512,\n",
    "        max_len=MAX_LEN,\n",
    "    ),\n",
    "    train_dataset=MedicalTranscriptionDataset(df=df_train),\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"out\",\n",
    "        per_device_train_batch_size=64,\n",
    "        num_train_epochs=8,\n",
    "        dataloader_num_workers=8,\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "model_path = f\"../../model/custom-bert/model.pt\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "# We'll save like any Pytorch model, as the model is not a Hugging Face `PreTrainedModel`\n",
    "torch.save(trainer.model, model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    y_true = preds.label_ids\n",
    "    y_pred = preds.predictions.argmax(-1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 572\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5594405594405595,\n",
       " 'precision': 0.48702118565027797,\n",
       " 'recall': 0.5594405594405595,\n",
       " 'f1': 0.503004396610791}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset=MedicalTranscriptionDataset(df=df_validation))\n",
    "compute_metrics(preds=preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-seminar-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
